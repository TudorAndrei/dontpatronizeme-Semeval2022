{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from ast import literal_eval\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os.path as osp\n",
    "from typing import Optional\n",
    "from transformer import RoBERTa\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPMDataset_submission(Dataset):\n",
    "    def __init__(self, model: str, data) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.max_len = 512\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        text = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "            \"ids\": torch.tensor(text[\"input_ids\"], dtype=torch.long),\n",
    "            \"mask\": torch.tensor(text[\"attention_mask\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = DontPatronizeMe('../dataset', test_path=\"../dataset/task4_test.tsv\")\n",
    "dpm.load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = DPMDataset_submission(model = \"mrm8488/distilroberta-finetuned-tweets-hate-speech\",data=dpm.test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset_, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrm8488/distilroberta-finetuned-tweets-hate-speech\n",
      "2048\n",
      "mrm8488/distilroberta-finetuned-tweets-hate-speech\n",
      "2048\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f2dad08fec1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "source": [
    "best_model_path = \"models/distillroberta_7/bert-val_loss0.47.ckpt\"\n",
    "hf_name = \"mrm8488/distilroberta-finetuned-tweets-hate-speech\"\n",
    "module = RoBERTa(model=hf_name).load_from_checkpoint(best_model_path, model=hf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission created\n"
     ]
    }
   ],
   "source": [
    "module.eval()\n",
    "module.freeze()\n",
    "preds = []\n",
    "file_path = \"../submissions/task2.txt\"\n",
    "for batch in data_loader:\n",
    "    ids,mask = batch[\"ids\"], batch[\"mask\"]\n",
    "    out = module(ids,mask)\n",
    "    predictions = torch.argmax(out, 1).numpy()\n",
    "    for i, pred in enumerate(predictions):\n",
    "        preds.append(pred)\n",
    "    \n",
    "    break\n",
    "result_df = pd.DataFrame(preds, columns=None)\n",
    "result_df.to_csv(file_path, index=False)\n",
    "print(\"submission created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.8737,   0.7811,  -0.6697,  -3.2605,  -3.0758,  -1.5863,  -6.0160],\n",
      "        [  4.8789,   0.4824,  -1.8092,  -2.2792,  -1.2954,  -2.4269,  -4.2241],\n",
      "        [ -1.7420,  -3.1297,  -1.0128,  -3.8017,  -0.6052,   4.0099,  -6.2521],\n",
      "        [  1.6155,  -0.8541,  -4.0080,  -2.8658,  -1.7835,  -2.0147,  -8.0312],\n",
      "        [  0.6495,  -4.7476,  -0.6911,  -1.3976,  -3.1453,  -0.5175,  -6.0527],\n",
      "        [ -2.4466,  -1.9455,   4.2553,   0.5320,  -4.6862,  -4.6971, -11.6382],\n",
      "        [  1.4569,  -1.7137,  -2.1345,  -4.1526,  -3.9218,  -0.3879,  -6.3081],\n",
      "        [  2.4493,  -2.0093,  -0.7099,   0.6080,  -2.2352,   0.5258,  -4.6004]])\n",
      "tensor([[9.4653e-01, 6.8591e-01, 3.3857e-01, 3.6950e-02, 4.4115e-02, 1.6990e-01,\n",
      "         2.4334e-03],\n",
      "        [9.9245e-01, 6.1831e-01, 1.4073e-01, 9.2863e-02, 2.1494e-01, 8.1145e-02,\n",
      "         1.4427e-02],\n",
      "        [1.4906e-01, 4.1900e-02, 2.6644e-01, 2.1846e-02, 3.5315e-01, 9.8219e-01,\n",
      "         1.9226e-03],\n",
      "        [8.3417e-01, 2.9858e-01, 1.7845e-02, 5.3870e-02, 1.4387e-01, 1.1767e-01,\n",
      "         3.2506e-04],\n",
      "        [6.5691e-01, 8.5977e-03, 3.3379e-01, 1.9820e-01, 4.1275e-02, 3.7344e-01,\n",
      "         2.3460e-03],\n",
      "        [7.9688e-02, 1.2505e-01, 9.8601e-01, 6.2995e-01, 9.1376e-03, 9.0395e-03,\n",
      "         8.8220e-06],\n",
      "        [8.1106e-01, 1.5269e-01, 1.0579e-01, 1.5481e-02, 1.9420e-02, 4.0422e-01,\n",
      "         1.8183e-03],\n",
      "        [9.2051e-01, 1.1823e-01, 3.2962e-01, 6.4748e-01, 9.6636e-02, 6.2851e-01,\n",
      "         9.9476e-03]])\n"
     ]
    }
   ],
   "source": [
    "module.eval()\n",
    "module.freeze()\n",
    "preds = []\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "file_path = \"../submissions/task2.txt\"\n",
    "for batch in data_loader:\n",
    "    ids,mask = batch[\"ids\"], batch[\"mask\"]\n",
    "    out = module(ids,mask)\n",
    "    print(out)\n",
    "    # predictions = torch.argmax(out, 1).numpy()\n",
    "    predictions = sigmoid(out)\n",
    "    print(predictions)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        preds.append(pred)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7359f9c874db4364c728011c1f4a97cf55832ccdeb70372c117f6582850e5526"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pcl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
