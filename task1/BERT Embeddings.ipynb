{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbf303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b765df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dont_patronize_me import DontPatronizeMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1a8678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>we 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>in libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"white house press secretary sean spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" just like we received migrants fleeing el ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  par_id      art_id    keyword country  \\\n",
       "0      1  @@24942188   hopeless      ph   \n",
       "1      2  @@21968160    migrant      gh   \n",
       "2      3  @@16584954  immigrant      ie   \n",
       "3      4   @@7811231   disabled      nz   \n",
       "4      5   @@1494111    refugee      ca   \n",
       "\n",
       "                                                text  label orig_label  \n",
       "0  we 're living in times of absolute insanity , ...      0          0  \n",
       "1  in libya today , there are countless number of...      0          0  \n",
       "2  \"white house press secretary sean spicer said ...      0          0  \n",
       "3  council customers only signs would be displaye...      0          0  \n",
       "4  \"\"\" just like we received migrants fleeing el ...      0          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm = DontPatronizeMe('.')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "# which we can then access as a dataframe\n",
    "dataset = dpm.train_task1_df\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc580fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9476\n",
       "1     993\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b31fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_subset.csv\")\n",
    "valid = pd.read_csv(\"validation_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e095eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    text = regrex_pattern.sub(r'',text)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Clear the special characters from our dataset\n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train['cleaned'] = [text_preprocessing(doc, True) for doc in train.text]\n",
    "valid['cleaned'] = [text_preprocessing(str(doc), True) for doc in valid.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8bb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.label.to_numpy()\n",
    "y_val = valid.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5484432c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5523677615090358, 1: 5.273929471032746}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a2aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\Raluca\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], output_shape=[512,16], \n",
    "  dtype=tf.string,trainable= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b3d3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c83ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dense_1 = 128, dense_2 = 64):\n",
    "  model = tf.keras.models.Sequential([\n",
    "  hub_layer,\n",
    "  tf.keras.layers.Dense(dense_1, activation='relu'),\n",
    "  tf.keras.layers.Dense(dense_2, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67192f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "131/131 [==============================] - 287s 2s/step - loss: 0.6262 - accuracy: 0.7131\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 287s 2s/step - loss: 0.3938 - accuracy: 0.8907\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 285s 2s/step - loss: 0.1214 - accuracy: 0.9816\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 283s 2s/step - loss: 0.0599 - accuracy: 0.9963\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 281s 2s/step - loss: 0.0561 - accuracy: 0.9975\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 267s 2s/step - loss: 0.0560 - accuracy: 0.9975\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 263s 2s/step - loss: 0.0560 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2443185fe50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "model = get_model(int(122), int(32.44))\n",
    "\n",
    "# Train the model for a specified number of epochs.\n",
    "optimizer = Adam(learning_rate = 0.001014)\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.fit(x = train.cleaned, y = y_train, epochs=7,\n",
    "              batch_size=64, class_weight=class_weights, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c14995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "498b84b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['REAL', 'FAKE']\n",
    "prediction_labels=[]\n",
    "for p in y_pred:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25170168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(valid.cleaned)\n",
    "classes = np.argmax(y_pred, axis = 1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55793b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b6a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9157593846321106, 0.8839541673660278]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x = valid.cleaned, y = y_val, steps=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28079c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1895,  199],\n",
       "       [   0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = confusion_matrix(classes, y_val)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83570eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(dense_1 = 128, dense_2 = 64):\n",
    "  model = tf.keras.models.Sequential([\n",
    "  hub_layer,\n",
    "  tf.keras.layers.Dense(dense_1, activation='relu'),\n",
    "  tf.keras.layers.Dense(dense_2, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "\n",
    "def fit_with(verbose, dense_1, dense_2, lr):\n",
    "\n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = get_model(int(dense_1), int(dense_2))\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer = Adam(learning_rate = lr)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    model.fit(x = train.cleaned, y = y_train, epochs=7,\n",
    "              batch_size=64, verbose=verbose)\n",
    "\n",
    "        # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(x = valid.cleaned, y = y_val, steps=10, verbose=0)\n",
    "    print('Loss:', score[0])\n",
    "    print('Accuracy:', score[1])\n",
    "\n",
    "    # Return the mae.\n",
    "\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10b40b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  dense_1  |  dense_2  |    lr     |\n",
      "-------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 0.1877 - accuracy: 0.9057\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0604 - accuracy: 0.9795\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0161 - accuracy: 0.9983\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0110 - accuracy: 0.9993\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0098 - accuracy: 0.9994\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0094 - accuracy: 0.9994\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0093 - accuracy: 0.9994\n",
      "Loss: 0.769253134727478\n",
      "Accuracy: 0.8882521390914917\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8883  \u001b[0m | \u001b[0m 144.1   \u001b[0m | \u001b[0m 96.68   \u001b[0m | \u001b[0m 0.000101\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.905   \u001b[0m | \u001b[95m 122.0   \u001b[0m | \u001b[95m 32.44   \u001b[0m | \u001b[95m 0.001014\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4303 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 99.76   \u001b[0m | \u001b[0m 54.7    \u001b[0m | \u001b[0m 0.004028\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 167.5   \u001b[0m | \u001b[0m 62.95   \u001b[0m | \u001b[0m 0.006884\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 0.2140 - accuracy: 0.9192\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0598 - accuracy: 0.9860\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0167 - accuracy: 0.9984\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0111 - accuracy: 0.9992\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0093 - accuracy: 0.9994\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0092 - accuracy: 0.9994\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0092 - accuracy: 0.9994\n",
      "Loss: 0.9465430974960327\n",
      "Accuracy: 0.88634192943573\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8863  \u001b[0m | \u001b[0m 103.3   \u001b[0m | \u001b[0m 114.3   \u001b[0m | \u001b[0m 0.000371\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 1.4404 - accuracy: 0.9058\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 192.7   \u001b[0m | \u001b[0m 62.74   \u001b[0m | \u001b[0m 0.005631\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 90.95   \u001b[0m | \u001b[0m 38.19   \u001b[0m | \u001b[0m 0.008027\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 1.4553 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 249.9   \u001b[0m | \u001b[0m 51.1    \u001b[0m | \u001b[0m 0.006954\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 0.2946 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.1119 - accuracy: 0.9467\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0083 - accuracy: 0.9993\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0074 - accuracy: 0.9995\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 0.0074 - accuracy: 0.9995\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 0.0074 - accuracy: 0.9995\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 0.0074 - accuracy: 0.9995\n",
      "Loss: 1.2021795511245728\n",
      "Accuracy: 0.8939828276634216\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.894   \u001b[0m | \u001b[0m 232.3   \u001b[0m | \u001b[0m 116.2   \u001b[0m | \u001b[0m 0.000941\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 71.5    \u001b[0m | \u001b[0m 35.02   \u001b[0m | \u001b[0m 0.008794\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 248.8   \u001b[0m | \u001b[0m 52.28   \u001b[0m | \u001b[0m 0.009041\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 190.0   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 1.4584 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 235.4   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 161.7   \u001b[0m | \u001b[0m 33.61   \u001b[0m | \u001b[0m 0.008598\u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 68.1    \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 0.2980 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.2128 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.1526 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0933 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0642 - accuracy: 0.9767\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0391 - accuracy: 0.9995\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 0.0151 - accuracy: 0.9996\n",
      "Loss: 0.6994386911392212\n",
      "Accuracy: 0.8949379324913025\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8949  \u001b[0m | \u001b[0m 212.8   \u001b[0m | \u001b[0m 37.93   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 1.4109 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 256.0   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 252s 2s/step - loss: 13.7024 - accuracy: 0.1008\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 13.8035 - accuracy: 0.0948\n",
      "Loss: 13.800049781799316\n",
      "Accuracy: 0.0950334295630455\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.09503 \u001b[0m | \u001b[0m 144.1   \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/7\n",
      "131/131 [==============================] - 253s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 250s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 249s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 1.4624 - accuracy: 0.9052\n",
      "Loss: 1.465885877609253\n",
      "Accuracy: 0.9049665927886963\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 180.1   \u001b[0m | \u001b[0m 40.94   \u001b[0m | \u001b[0m 0.009382\u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "131/131 [==============================] - 254s 2s/step - loss: 0.3045 - accuracy: 0.9052\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.2687 - accuracy: 0.9052\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.2084 - accuracy: 0.9052\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.1536 - accuracy: 0.9052\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.1061 - accuracy: 0.9052\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0708 - accuracy: 0.9543\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - 251s 2s/step - loss: 0.0431 - accuracy: 0.9994\n",
      "Loss: 0.6320094466209412\n",
      "Accuracy: 0.8887296915054321\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8887  \u001b[0m | \u001b[0m 124.3   \u001b[0m | \u001b[0m 55.48   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n",
      "=============================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.8882521390914917, 'params': {'dense_1': 144.0682249028942, 'dense_2': 96.67634326552171, 'lr': 0.00010113231069171439}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 122.04785394531324, 'dense_2': 32.43665977151666, 'lr': 0.0010141520882110983}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 99.76196058451282, 'dense_2': 54.702801428821346, 'lr': 0.004027997994883633}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 167.45281292864453, 'dense_2': 62.94978561316902, 'lr': 0.00688367305392792}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.88634192943573, 'params': {'dense_1': 103.25483194845134, 'dense_2': 114.34915287578589, 'lr': 0.00037113717265946903}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 192.72976195425323, 'dense_2': 62.73813786511822, 'lr': 0.005631029301612942}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 90.95429221028488, 'dense_2': 38.187366777506426, 'lr': 0.008027371229887814}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 249.90622253812433, 'dense_2': 51.1035079538352, 'lr': 0.0069539938951262105}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.8939828276634216, 'params': {'dense_1': 232.26671724083934, 'dense_2': 116.1959463124309, 'lr': 0.0009419376925608015}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 71.49851838071342, 'dense_2': 35.021006991231715, 'lr': 0.00879361078395119}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 248.77907733777607, 'dense_2': 52.280912599998786, 'lr': 0.009040813633319357}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 189.99711558645538, 'dense_2': 16.0, 'lr': 0.01}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 235.38398590158175, 'dense_2': 16.0, 'lr': 0.01}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 161.72844179050415, 'dense_2': 33.608532876229624, 'lr': 0.008598211397521937}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 64.0, 'dense_2': 68.09976774191904, 'lr': 0.0001}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.8949379324913025, 'params': {'dense_1': 212.8408736443038, 'dense_2': 37.93204282135724, 'lr': 0.0001}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 256.0, 'dense_2': 16.0, 'lr': 0.01}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.0950334295630455, 'params': {'dense_1': 144.05036409455414, 'dense_2': 16.0, 'lr': 0.01}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.9049665927886963, 'params': {'dense_1': 180.130964933439, 'dense_2': 40.944741859080196, 'lr': 0.009381600459852334}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.8887296915054321, 'params': {'dense_1': 124.3029600167593, 'dense_2': 55.47662500351157, 'lr': 0.0001}}\n",
      "{'target': 0.9049665927886963, 'params': {'dense_1': 122.04785394531324, 'dense_2': 32.43665977151666, 'lr': 0.0010141520882110983}}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with, verbose)\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dense_1': (64, 256), 'dense_2': (16, 128), 'lr': (1e-4, 1e-2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=10)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "train = pd.read_csv(\"train_subset.csv\")\n",
    "valid = pd.read_csv(\"validation_subset.csv\")\n",
    "\n",
    "def text_preprocessing(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Clear the special characters from our dataset\n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    text = ' '.join(text)\n",
    "    #print('\\tClear Text.\\n', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #print('\\tTokenizing.\\n', tokens)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in set(stop)]\n",
    "    #print('\\tRemoving the stopwords.\\n', tokens)\n",
    "    \n",
    "    # Remove words shorter than 3 characters\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "    #print('\\tRemoving the words shorter than 3 characters\\n', tokens)\n",
    "    \n",
    "\n",
    "    if lemmatize:\n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        #print('\\tLemmatizing.\\n', tokens)\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Preprocessed text\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "'''def text_preprocessing(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    text = regrex_pattern.sub(r'',text)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text'''\n",
    "\n",
    "train['cleaned'] = [text_preprocessing(doc, True) for doc in train.text]\n",
    "valid['cleaned'] = [text_preprocessing(str(doc), True) for doc in valid.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=1,\n",
    "                                   max_df=0.7, # Removing frequent words like 'subject' that appear in every news\n",
    "                                   ngram_range=(1,2), \n",
    "                                   lowercase=False, # we've already lowercased the text\n",
    "                                   stop_words='english', \n",
    "                                   max_features=1340000)\n",
    "X_train = vectorizer_tfidf.fit_transform(train.cleaned)\n",
    "X_valid = vectorizer_tfidf.transform(valid.cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.label.to_numpy()\n",
    "y_val = valid.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1be15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_paced_ensemble import SelfPacedEnsembleClassifier\n",
    "from self_paced_ensemble.self_paced_ensemble.base import sort_dict_by_key\n",
    "from self_paced_ensemble.utils._plot import plot_2Dprojection_and_cardinality\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5074bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2797814207650273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.95      0.80      1378\n",
      "           1       0.64      0.18      0.28       716\n",
      "\n",
      "    accuracy                           0.69      2094\n",
      "   macro avg       0.67      0.56      0.54      2094\n",
      "weighted avg       0.67      0.69      0.62      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "print(f1_score(y_pred, y_val))\n",
    "print(classification_report(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d39401e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3120260021668472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.81      1370\n",
      "           1       0.72      0.20      0.31       724\n",
      "\n",
      "    accuracy                           0.70      2094\n",
      "   macro avg       0.71      0.58      0.56      2094\n",
      "weighted avg       0.70      0.70      0.63      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=RandomForestClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid)\n",
    "print(f1_score(y_pred, y_val))\n",
    "print(classification_report(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "939ca0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82915356",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c01ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(train.text).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "210919e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in train.text:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29515dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )\n",
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a50529e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]\n",
    "feats = get_feature_array(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cdf4fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8375, 3933)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3c7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d7232b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = train.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8e97ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l2\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0577117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb5f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1efc0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142079477408819\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_preds, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc282ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
