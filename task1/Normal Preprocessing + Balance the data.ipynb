{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1efa7b",
   "metadata": {},
   "source": [
    "## <center>Task 1 : Don't patronize me</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "33656872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Imblearn methods\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\n",
    "\n",
    "# Text augmentation techniques\n",
    "from textaugment import MIXUP\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#https://github.com/learn-co-students/dsc-classification-with-word-embeddings-codealong-online-ds-pt-100719\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from self_paced_ensemble import SelfPacedEnsembleClassifier\n",
    "from self_paced_ensemble.self_paced_ensemble.base import sort_dict_by_key\n",
    "from self_paced_ensemble.utils._plot import plot_2Dprojection_and_cardinality\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "389b0211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>we 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>in libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"white house press secretary sean spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" just like we received migrants fleeing el ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  par_id      art_id    keyword country  \\\n",
       "0      1  @@24942188   hopeless      ph   \n",
       "1      2  @@21968160    migrant      gh   \n",
       "2      3  @@16584954  immigrant      ie   \n",
       "3      4   @@7811231   disabled      nz   \n",
       "4      5   @@1494111    refugee      ca   \n",
       "\n",
       "                                                text  label orig_label  \n",
       "0  we 're living in times of absolute insanity , ...      0          0  \n",
       "1  in libya today , there are countless number of...      0          0  \n",
       "2  \"white house press secretary sean spicer said ...      0          0  \n",
       "3  council customers only signs would be displaye...      0          0  \n",
       "4  \"\"\" just like we received migrants fleeing el ...      0          0  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm = DontPatronizeMe('.')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "# which we can then access as a dataframe\n",
    "dataset = dpm.train_task1_df\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "53e222b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_subset.csv\")\n",
    "valid = pd.read_csv(\"validation_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b402467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Class 1: 7581 - 794\n"
     ]
    }
   ],
   "source": [
    "print(\"Class 0 - Class 1: {} - {}\".format(len(train[train.label == 0]), len(train[train.label == 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3351d2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "e3417491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Clear the special characters from our dataset\n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    text = ' '.join(text)\n",
    "    #print('\\tClear Text.\\n', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #print('\\tTokenizing.\\n', tokens)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in set(stop)]\n",
    "    #print('\\tRemoving the stopwords.\\n', tokens)\n",
    "    \n",
    "    # Remove words shorter than 3 characters\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "    #print('\\tRemoving the words shorter than 3 characters\\n', tokens)\n",
    "    \n",
    "\n",
    "    if lemmatize:\n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        #print('\\tLemmatizing.\\n', tokens)\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Preprocessed text\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "train['cleaned_lemma'] = [text_preprocessing(doc, True) for doc in train.text]\n",
    "valid['cleaned_lemma'] = [text_preprocessing(str(doc), True) for doc in valid.text]\n",
    "\n",
    "train['cleaned_stem'] = [text_preprocessing(doc, False) for doc in train.text]\n",
    "valid['cleaned_stem'] = [text_preprocessing(str(doc), False) for doc in valid.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "b4c630b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features = 2000)\n",
    "\n",
    "X_train_lemma = vectorizer_tfidf.fit_transform(train.cleaned_lemma)\n",
    "X_valid_lemma = vectorizer_tfidf.transform(valid.cleaned_lemma)\n",
    "\n",
    "X_train_stem = vectorizer_tfidf.fit_transform(train.cleaned_stem)\n",
    "X_valid_stem = vectorizer_tfidf.transform(valid.cleaned_stem)\n",
    "\n",
    "\n",
    "# Balancing the data\n",
    "X_train_lemma_SMOTE, y_train_lemma_SMOTE = SMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_ROS, y_train_lemma_ROS = RandomOverSampler(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_ADASYN, y_train_lemma_ADASYN = ADASYN(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_BorderlineSMOTE, y_train_lemma_BorderlineSMOTE = BorderlineSMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE = SVMSMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "\n",
    "# Balancing the data\n",
    "X_train_stem_SMOTE, y_train_stem_SMOTE = SMOTE(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_ROS, y_train_stem_ROS = RandomOverSampler(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_ADASYN, y_train_stem_ADASYN = ADASYN(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_BorderlineSMOTE, y_train_stem_BorderlineSMOTE = BorderlineSMOTE(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE = SVMSMOTE(random_state = 42).fit_resample(X_train_stem, train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724517a",
   "metadata": {},
   "source": [
    "### Deep Learning model for dataset unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "e1749e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "    \n",
    "    return class_weight\n",
    "\n",
    "def get_f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "class_weights = create_class_weight({0: 7581, 1: 794})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095cc587",
   "metadata": {},
   "source": [
    "- X_train_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "43826996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_228 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_230 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_231 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 3s 3ms/step - loss: 0.3028 - get_f1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.2051 - get_f1: 0.3033\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.1292 - get_f1: 0.6412\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0480 - get_f1: 0.8507\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0091 - get_f1: 0.9395\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0017 - get_f1: 0.9491\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 6.9940e-04 - get_f1: 0.9422\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.6336e-04 - get_f1: 0.9695\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.2898e-05 - get_f1: 0.9809\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.3288e-05 - get_f1: 0.9618\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 9.0672e-06 - get_f1: 0.9389\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 6.6109e-06 - get_f1: 0.9618\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 4.9679e-06 - get_f1: 0.9580\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 3.8370e-06 - get_f1: 0.9504\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.9935e-06 - get_f1: 0.9656\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.3593e-06 - get_f1: 0.9427\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.8833e-06 - get_f1: 0.9695\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.5085e-06 - get_f1: 0.9466\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.2208e-06 - get_f1: 0.9733\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 9.7862e-07 - get_f1: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252e7c4c3d0>"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma.toarray(), train.label, epochs = 20, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "45c82230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "y_pred\n",
    "\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601fdbb",
   "metadata": {},
   "source": [
    "- X_train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "d6084a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_233 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.2980 - get_f1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.2015 - get_f1: 0.3168\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.1319 - get_f1: 0.6225\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0588 - get_f1: 0.8207\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0181 - get_f1: 0.9032\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0055 - get_f1: 0.9598\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0018 - get_f1: 0.9758\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 1.7011e-04 - get_f1: 0.9275\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.9580e-05 - get_f1: 0.9542\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.6081e-05 - get_f1: 0.9618\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.0114e-05 - get_f1: 0.9618\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 6.9352e-06 - get_f1: 0.9504\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.0357e-06 - get_f1: 0.9733\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 3.8049e-06 - get_f1: 0.9733\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.9558e-06 - get_f1: 0.9542\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.3174e-06 - get_f1: 0.9542\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.8535e-06 - get_f1: 0.9695\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.4949e-06 - get_f1: 0.9733\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.2051e-06 - get_f1: 0.9542\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 9.7979e-07 - get_f1: 0.9656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d5987fa0>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem.toarray(), train.label, epochs = 20, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "5945c418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2698412698412698"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c422b",
   "metadata": {},
   "source": [
    "### Deep Learning model for oversampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cf559",
   "metadata": {},
   "source": [
    "- Lemmatized oversampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8cd6093e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_238 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2213 - get_f1: 0.8744\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0149 - get_f1: 0.9952\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0013 - get_f1: 0.9998\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3557e-04 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.0467e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.1615e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.3752e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.8504e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8819e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4212e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0103e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4766e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.6045e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0714e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1316e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2888e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7318e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2923e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 9.7834e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4075e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d998b790>"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_SMOTE.toarray(), y_train_lemma_SMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "c9a448d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28228228228228225"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b9e41340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_243 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2703 - get_f1: 0.8890\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0227 - get_f1: 0.9932\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9997: 0s - loss: 0\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 3.0904e-05 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2834e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.5948e-06 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8720e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2838e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2853e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6244e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.1711e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.5271e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.2159e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.5284e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2901e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.4114e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7822e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3326e-07 - get_f1: 1.0000: 0s - loss: 1\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0098e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.7200e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d3ef9310>"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_ROS.toarray(), y_train_lemma_ROS, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "d2119205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21453287197231835"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "acabfe35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_248 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "467/467 [==============================] - 2s 3ms/step - loss: 0.2107 - get_f1: 0.8803\n",
      "Epoch 2/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0131 - get_f1: 0.9963\n",
      "Epoch 3/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9986\n",
      "Epoch 4/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.3706e-05 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.0509e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.1418e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.2103e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.2572e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.9953e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.1533e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.8387e-07 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 5.8356e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.3775e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 3.2648e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.6513e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.0140e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.6084e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.2424e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.8071e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.7881e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251e10bae50>"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_ADASYN.toarray(), y_train_lemma_ADASYN, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "f9367417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2774566473988439"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "7f3c8d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_253 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_255 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2269 - get_f1: 0.8959\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0347 - get_f1: 0.9906\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0064 - get_f1: 0.9981\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9989\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0040 - get_f1: 0.9989\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9987\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0021 - get_f1: 0.9989\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9989\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.2061e-04 - get_f1: 0.9997\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0242e-05 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8380e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.1941e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0844e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.7700e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.5533e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1077e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.0897e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3609e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.8228e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dd544160>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_BorderlineSMOTE.toarray(), y_train_lemma_BorderlineSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "cc0fa02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19292604501607716"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "5ec5e43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_258 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_260 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_261 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2325 - get_f1: 0.9048\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0527 - get_f1: 0.9849\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0169 - get_f1: 0.9948\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0051 - get_f1: 0.9983\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0039 - get_f1: 0.9987\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9988\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0029 - get_f1: 0.9990\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 9.7769e-04 - get_f1: 0.9999\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9989\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0025 - get_f1: 0.9989\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.5262e-05 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.4433e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.9319e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3916e-06 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0270e-06 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.6499e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.7165e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.2802e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1943e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dc6b71c0>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_SVMSMOTE.toarray(), y_train_lemma_SVMSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "d2a732e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3187660668380463"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d288519",
   "metadata": {},
   "source": [
    "- Stemmed oversampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0a4583a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_263 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2156 - get_f1: 0.9113\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0186 - get_f1: 0.9950\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0028 - get_f1: 0.9992\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0029 - get_f1: 0.9990\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.3316e-04 - get_f1: 0.9999\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9987\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0049 - get_f1: 0.9986\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0065 - get_f1: 0.9980\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0020 - get_f1: 0.9994\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.1098e-05 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5748e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.4686e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.6966e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.3555e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5435e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0374e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.2289e-08 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.1833e-08 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.8549e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9383e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dc7c0970>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_SMOTE.toarray(), y_train_stem_SMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "5a6c9460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28901734104046245"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "0ef3d766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_268 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2548 - get_f1: 0.8630\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0195 - get_f1: 0.9942\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0017 - get_f1: 0.9995\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4924e-04 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9705e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7013e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0265e-05 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.9538e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8870e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.4511e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.5262e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8176e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3463e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0045e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4582e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.6091e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.3291e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2648e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.4811e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8974e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252e5efafa0>"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_ROS.toarray(), y_train_stem_ROS, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "16aab7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2922077922077922"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "2e30d6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_278 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2049 - get_f1: 0.8711\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0125 - get_f1: 0.9962\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0023 - get_f1: 0.9993\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0019 - get_f1: 0.9974\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.4966e-05 - get_f1: 0.9979\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2944e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4894e-06 - get_f1: 0.9979\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8300e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2742e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2476e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4648e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.9334e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.7101e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.9576e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9078e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.1927e-07 - get_f1: 0.9979\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6797e-07 - get_f1: 0.9979\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2995e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0118e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.9387e-08 - get_f1: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dd180910>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_ADASYN.toarray(), y_train_stem_ADASYN, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "94d0a8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2821316614420063"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "62283002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_283 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2203 - get_f1: 0.9126\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0347 - get_f1: 0.9898\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0087 - get_f1: 0.9974\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9990\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0319e-04 - get_f1: 0.9999\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0065 - get_f1: 0.9978\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0039 - get_f1: 0.9988\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0014 - get_f1: 0.9992\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0017 - get_f1: 0.9995\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2841e-05 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.5652e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5436e-06 - get_f1: 1.0000: 0s - loss: 1.5365e\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0679e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.6195e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.5385e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0730e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.0208e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2583e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6969e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d36577f0>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_BorderlineSMOTE.toarray(), y_train_stem_BorderlineSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "6e17e8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28648648648648645"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "dacb0bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_288 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_291 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2141 - get_f1: 0.9009\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0443 - get_f1: 0.9873\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0135 - get_f1: 0.9957\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9985\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0041 - get_f1: 0.9987\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0010 - get_f1: 0.9996\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0067 - get_f1: 0.9974\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9990\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0012 - get_f1: 0.9996\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0011 - get_f1: 0.9996\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3890e-05 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.5751e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6556e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2434e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.8979e-08 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.9055e-08 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.5421e-08 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.7963e-08 - get_f1: 1.0000: 0s - loss: 2.862\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.3165e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251e1b10580>"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_SVMSMOTE.toarray(), y_train_stem_SVMSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "86ee65f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31666666666666665"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2369ea9",
   "metadata": {},
   "source": [
    "## Tokenizer and pad sequence, followed by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "19cea95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 15s 52ms/step - loss: 0.3616 - accuracy: 0.9019 - get_f1: 4.4903e-04\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 14s 52ms/step - loss: 0.2661 - accuracy: 0.9079 - get_f1: 0.1214\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 14s 54ms/step - loss: 0.2012 - accuracy: 0.9280 - get_f1: 0.4351\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 14s 53ms/step - loss: 0.1600 - accuracy: 0.9442 - get_f1: 0.6029\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1263 - accuracy: 0.9587 - get_f1: 0.6941\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.0945 - accuracy: 0.9717 - get_f1: 0.7663\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.0659 - accuracy: 0.9820 - get_f1: 0.8634\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0493 - accuracy: 0.9883 - get_f1: 0.8826\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0406 - accuracy: 0.9915 - get_f1: 0.9153\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0312 - accuracy: 0.9944 - get_f1: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23170731707317074"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train.cleaned_lemma))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train.cleaned_lemma)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(valid.cleaned_lemma)\n",
    "\n",
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "rnn_tiny = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_tiny.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_tiny.add(layers.LSTM(128, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_tiny.add(layers.GlobalMaxPool1D()) \n",
    "rnn_tiny.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1(.0001) ))\n",
    "rnn_tiny.add(Dropout(0.1))\n",
    "rnn_tiny.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_tiny.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy', get_f1])\n",
    "history_tiny_rnn = rnn_tiny.fit(X_t, train.label, epochs=10)\n",
    "\n",
    "y = rnn_tiny.predict(X_te)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "92e14a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 17s 55ms/step - loss: 0.3548 - accuracy: 0.9052 - get_f1: 0.0000e+00\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.2482 - accuracy: 0.9151 - get_f1: 0.2248\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.1897 - accuracy: 0.9321 - get_f1: 0.4917\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.1457 - accuracy: 0.9503 - get_f1: 0.6494\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.1073 - accuracy: 0.9655 - get_f1: 0.7447\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0811 - accuracy: 0.9764 - get_f1: 0.8309\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0552 - accuracy: 0.9869 - get_f1: 0.8799\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 16s 59ms/step - loss: 0.0411 - accuracy: 0.9904 - get_f1: 0.9122\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0306 - accuracy: 0.9946 - get_f1: 0.9233\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0296 - accuracy: 0.9940 - get_f1: 0.9193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2789473684210526"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train.cleaned_stem))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train.cleaned_stem)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(valid.cleaned_stem)\n",
    "\n",
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "rnn_tiny = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_tiny.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_tiny.add(layers.LSTM(128, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_tiny.add(layers.GlobalMaxPool1D()) \n",
    "rnn_tiny.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1(.0001) ))\n",
    "rnn_tiny.add(Dropout(0.1))\n",
    "rnn_tiny.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_tiny.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy', get_f1])\n",
    "history_tiny_rnn = rnn_tiny.fit(X_t, train.label, epochs=10)\n",
    "\n",
    "y = rnn_tiny.predict(X_te)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953263ad",
   "metadata": {},
   "source": [
    "## Datasets applied on ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "cc55feb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.5899960614415124\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.599647266313933\n",
      "F1 score for XGB on train is  0.6633081444164567\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.34076433121019106\n",
      "F1 score for Random Forest on validation is  0.057971014492753624\n",
      "F1 score for SVM on validation is  0.057971014492753624\n",
      "F1 score for XGB on validation is  0.15510204081632653\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.6011976047904192\n",
      "F1 score for Random Forest on train is  0.9993698802772526\n",
      "F1 score for SVM on train is  0.9808523780111179\n",
      "F1 score for XGB on train is  0.7047308319738989\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.35197368421052627\n",
      "F1 score for Random Forest on validation is  0.01990049751243781\n",
      "F1 score for SVM on validation is  0.2756183745583039\n",
      "F1 score for XGB on validation is  0.1721311475409836\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "rf = RandomForestClassifier(class_weight = 'balanced')\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma, train.label)\n",
    "rf.fit(X_train_lemma, train.label)\n",
    "svm.fit(X_train_lemma, train.label)\n",
    "xgb.fit(X_train_lemma, train.label)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma), train.label))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC(class_weight = 'balanced')\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem, train.label)\n",
    "rf.fit(X_train_stem, train.label)\n",
    "svm.fit(X_train_stem, train.label)\n",
    "xgb.fit(X_train_stem, train.label)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem), train.label))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "6fc35be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:36:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9276644863409004\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9978187586753916\n",
      "F1 score for XGB on train is  0.9716719626800081\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.34676007005253934\n",
      "F1 score for Random Forest on validation is  0.20610687022900764\n",
      "F1 score for SVM on validation is  0.10138248847926268\n",
      "F1 score for XGB on validation is  0.23225806451612904\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:37:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9316472545757072\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9930942895086322\n",
      "F1 score for XGB on train is  0.9713319810682893\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.350561797752809\n",
      "F1 score for Random Forest on validation is  0.15833333333333333\n",
      "F1 score for SVM on validation is  0.13392857142857142\n",
      "F1 score for XGB on validation is  0.23948220064724918\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "rf.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "svm.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "xgb.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "rf.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "svm.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "xgb.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "07c89220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9310211946050096\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9982160555004956\n",
      "F1 score for XGB on train is  0.9723197407507427\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.38007380073800734\n",
      "F1 score for Random Forest on validation is  0.2007722007722008\n",
      "F1 score for SVM on validation is  0.13636363636363635\n",
      "F1 score for XGB on validation is  0.225705329153605\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:41:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9418673705047839\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9946290033817387\n",
      "F1 score for XGB on train is  0.9728598433702403\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.3744493392070484\n",
      "F1 score for Random Forest on validation is  0.12605042016806722\n",
      "F1 score for SVM on validation is  0.19742489270386268\n",
      "F1 score for XGB on validation is  0.2382445141065831\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "rf.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "svm.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "xgb.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "rf.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "svm.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "xgb.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c518c6a",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "154fa1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 11815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9185609, 10171650)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_lemma = [_text.split() for _text in train.cleaned_lemma]\n",
    "\n",
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size = 300, \n",
    "                                            window = 2, \n",
    "                                            min_count = 2)\n",
    "w2v_model.build_vocab(documents_lemma)\n",
    "words = w2v_model.wv.index_to_key\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)\n",
    "w2v_model.train(documents_lemma, total_examples=len(documents_lemma), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c0e36ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 23228\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.cleaned_lemma)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train.cleaned_lemma), maxlen = 300)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(valid.cleaned_lemma), maxlen = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "b4c9ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23228, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "fd733b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_37 (Embedding)    (None, 300, 300)          6968400   \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128)               219648    \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,188,177\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 6,968,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 42s 154ms/step - loss: 0.2920 - get_f1: 0.0248\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 45s 171ms/step - loss: 0.2495 - get_f1: 0.1290\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 42s 159ms/step - loss: 0.2275 - get_f1: 0.2348\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 43s 162ms/step - loss: 0.1991 - get_f1: 0.3650\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.1639 - get_f1: 0.5181\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 43s 162ms/step - loss: 0.1172 - get_f1: 0.6559\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.0727 - get_f1: 0.7940\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 42s 158ms/step - loss: 0.0405 - get_f1: 0.8836\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.0243 - get_f1: 0.9057\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 43s 163ms/step - loss: 0.0114 - get_f1: 0.9411\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 42s 160ms/step - loss: 0.0044 - get_f1: 0.9537\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 42s 162ms/step - loss: 0.0018 - get_f1: 0.9618\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 44s 169ms/step - loss: 8.4535e-04 - get_f1: 0.9542\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 44s 167ms/step - loss: 5.5656e-04 - get_f1: 0.9580\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 46s 175ms/step - loss: 3.9885e-04 - get_f1: 0.9542\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 43s 166ms/step - loss: 2.9976e-04 - get_f1: 0.9427\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 43s 163ms/step - loss: 2.2741e-04 - get_f1: 0.9389\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 46s 176ms/step - loss: 1.7929e-04 - get_f1: 0.9466\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 42s 162ms/step - loss: 1.4044e-04 - get_f1: 0.9580\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 42s 160ms/step - loss: 1.1318e-04 - get_f1: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23076923076923075"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[get_f1])\n",
    "history = model.fit(x_train, train.label,\n",
    "                    batch_size=32,\n",
    "                    epochs=20,\n",
    "                    verbose=1)\n",
    "\n",
    "y = model.predict(x_test)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60603d22",
   "metadata": {},
   "source": [
    "### Self paced ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "3a2aed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43447332421340623\n",
      "0.3132036847492323\n"
     ]
    }
   ],
   "source": [
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=RandomForestClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train_lemma, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid_lemma)\n",
    "print(f1_score(clf.predict(X_train_lemma), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "15dcac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41386499869689863\n",
      "0.30931796349663787\n"
     ]
    }
   ],
   "source": [
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=RandomForestClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train_stem, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid_stem)\n",
    "print(f1_score(clf.predict(X_train_stem), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b74dd",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "e9a76d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Class 1: 7581 - 6372\n"
     ]
    }
   ],
   "source": [
    "hate1 = pd.read_csv(\"hate1.csv\")\n",
    "hate1 = hate1[hate1.label == 1]\n",
    "hate1 = hate1.rename({'tweet': 'text'}, axis = 1)\n",
    "\n",
    "hate2 = pd.read_excel(\"hate2.xlsx\")\n",
    "hate2 = hate2[hate2.Label == 1]\n",
    "hate2 = hate2.rename({'Sentences': 'text', 'Label': 'label'}, axis = 1)\n",
    "\n",
    "hate3 = pd.read_csv('english_dataset.tsv',sep='\\t')\n",
    "hate3 = hate3[hate3['task_1'] == 'HOF']\n",
    "hate3 = hate3.rename({'task_1': 'label'}, axis = 1)\n",
    "\n",
    "hate4 = pd.read_csv('hasoc2019_en_test-2919.tsv', sep = '\\t')\n",
    "hate4 = hate4[hate4['task_1'] == 'HOF']\n",
    "hate4 = hate4.rename({'task_1': 'label'}, axis = 1)\n",
    "\n",
    "hate5 = pd.read_csv('hate5.csv', sep = ';')\n",
    "hate5.isHate = hate5.isHate.astype(int)\n",
    "hate5 = hate5[hate5['isHate'] == 1]\n",
    "hate5 = hate5.rename({'comment': 'text', 'isHate': 'label'}, axis = 1)\n",
    "\n",
    "train = pd.concat([train, hate1, hate2, hate3, hate4, hate5])\n",
    "train = train[['text', 'label']]\n",
    "train.label = train.label.map({0: 0, 1: 1, 'HOF': 1})\n",
    "print(\"Class 0 - Class 1: {} - {}\".format(len(train[train.label == 0]), len(train[train.label == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "27c97925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "      <td>scheme saw estimated child poor family sent pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "      <td>durban homeless community reconciliation lunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "      <td>next immediate problem cropped assist unfortun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "      <td>far important implication economy god dollar l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "      <td>strengthen child sensitive social protection s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>All muslims worship Satan¬ü¬ï¬ã 5 times a day</td>\n",
       "      <td>1</td>\n",
       "      <td>muslim worship satan time day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>all mental illnesses are awful and must be tre...</td>\n",
       "      <td>1</td>\n",
       "      <td>mental illness awful must treated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>All girls like you should have a pimp to guide...</td>\n",
       "      <td>1</td>\n",
       "      <td>girl like pimp guide successful prostitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>After all this time the african americans are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>time african american people crime rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>actually autism can make you a retard</td>\n",
       "      <td>1</td>\n",
       "      <td>actually autism make retard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13953 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "0    the scheme saw an estimated 150,000 children f...      1   \n",
       "1    durban 's homeless communities reconciliation ...      1   \n",
       "2    the next immediate problem that cropped up was...      1   \n",
       "3    far more important than the implications for t...      1   \n",
       "4    to strengthen child-sensitive social protectio...      1   \n",
       "..                                                 ...    ...   \n",
       "158      All muslims worship Satan¬ü¬ï¬ã 5 times a day      1   \n",
       "159  all mental illnesses are awful and must be tre...      1   \n",
       "160  All girls like you should have a pimp to guide...      1   \n",
       "161  After all this time the african americans are ...      1   \n",
       "162              actually autism can make you a retard      1   \n",
       "\n",
       "                                         cleaned_lemma  \n",
       "0    scheme saw estimated child poor family sent pa...  \n",
       "1       durban homeless community reconciliation lunch  \n",
       "2    next immediate problem cropped assist unfortun...  \n",
       "3    far important implication economy god dollar l...  \n",
       "4    strengthen child sensitive social protection s...  \n",
       "..                                                 ...  \n",
       "158                      muslim worship satan time day  \n",
       "159                  mental illness awful must treated  \n",
       "160       girl like pimp guide successful prostitution  \n",
       "161            time african american people crime rate  \n",
       "162                        actually autism make retard  \n",
       "\n",
       "[13953 rows x 3 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_lemma'] = [text_preprocessing(doc, True) for doc in train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "be9a57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352062357908413\n",
      "0.23003194888178913\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features = 5000)\n",
    "\n",
    "X_train_lemma = vectorizer_tfidf.fit_transform(train.cleaned_lemma)\n",
    "X_valid_lemma = vectorizer_tfidf.transform(valid.cleaned_lemma)\n",
    "\n",
    "# Train an SPE classifier\n",
    "lr2 = LogisticRegression(class_weight = 'balanced')\n",
    "lr2.fit(X_train_lemma, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = lr2.predict(X_valid_lemma)\n",
    "print(f1_score(lr2.predict(X_train_lemma), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "7e2e4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = create_class_weight({0: 7581, 1: 6372})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "f458b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_319 (Dense)           (None, 32)                160032    \n",
      "                                                                 \n",
      " dense_320 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 341,281\n",
      "Trainable params: 341,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "437/437 [==============================] - 3s 4ms/step - loss: 0.2777 - get_f1: 0.8602\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.1306 - get_f1: 0.9442\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0576 - get_f1: 0.9740\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0227 - get_f1: 0.9876\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0095 - get_f1: 0.9969\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0045 - get_f1: 0.9965\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0012 - get_f1: 0.9997\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0017 - get_f1: 0.9973\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 7.9105e-04 - get_f1: 0.9998\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0016 - get_f1: 0.9995\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 6.6591e-04 - get_f1: 0.9976\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0028 - get_f1: 0.9991\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0070 - get_f1: 0.9951\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0048 - get_f1: 0.9980\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 9.9921e-04 - get_f1: 0.9975\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 3.3615e-04 - get_f1: 0.9999\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.6653e-04 - get_f1: 0.9999\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.4647e-04 - get_f1: 0.9976\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.1354e-04 - get_f1: 0.9999\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.0457e-04 - get_f1: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21553884711779447"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 5000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma.toarray(), train.label, epochs = 20, class_weight = class_weights)\n",
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741f61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
