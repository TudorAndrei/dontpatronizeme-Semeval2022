{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aefb286",
   "metadata": {},
   "source": [
    "## <center>Task 1 : Don't patronize me</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "33656872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Imblearn methods\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\n",
    "\n",
    "# Text augmentation techniques\n",
    "from textaugment import MIXUP\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#https://github.com/learn-co-students/dsc-classification-with-word-embeddings-codealong-online-ds-pt-100719\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from self_paced_ensemble import SelfPacedEnsembleClassifier\n",
    "from self_paced_ensemble.self_paced_ensemble.base import sort_dict_by_key\n",
    "from self_paced_ensemble.utils._plot import plot_2Dprojection_and_cardinality\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "389b0211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>we 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>in libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"white house press secretary sean spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" just like we received migrants fleeing el ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  par_id      art_id    keyword country  \\\n",
       "0      1  @@24942188   hopeless      ph   \n",
       "1      2  @@21968160    migrant      gh   \n",
       "2      3  @@16584954  immigrant      ie   \n",
       "3      4   @@7811231   disabled      nz   \n",
       "4      5   @@1494111    refugee      ca   \n",
       "\n",
       "                                                text  label orig_label  \n",
       "0  we 're living in times of absolute insanity , ...      0          0  \n",
       "1  in libya today , there are countless number of...      0          0  \n",
       "2  \"white house press secretary sean spicer said ...      0          0  \n",
       "3  council customers only signs would be displaye...      0          0  \n",
       "4  \"\"\" just like we received migrants fleeing el ...      0          0  "
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm = DontPatronizeMe('.')\n",
    "# This method loads the subtask 1 data\n",
    "dpm.load_task1()\n",
    "# which we can then access as a dataframe\n",
    "dataset = dpm.train_task1_df\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "53e222b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_subset.csv\")\n",
    "valid = pd.read_csv(\"validation_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "d2512c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Class 1: 7581 - 794\n"
     ]
    }
   ],
   "source": [
    "print(\"Class 0 - Class 1: {} - {}\".format(len(train[train.label == 0]), len(train[train.label == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "3a89619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1895\n",
       "1     199\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09fcca",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "43000451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Clear the special characters from our dataset\n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    text = ' '.join(text)\n",
    "    #print('\\tClear Text.\\n', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #print('\\tTokenizing.\\n', tokens)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in set(stop)]\n",
    "    #print('\\tRemoving the stopwords.\\n', tokens)\n",
    "    \n",
    "    # Remove words shorter than 3 characters\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "    #print('\\tRemoving the words shorter than 3 characters\\n', tokens)\n",
    "    \n",
    "\n",
    "    if lemmatize:\n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        #print('\\tLemmatizing.\\n', tokens)\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Preprocessed text\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "train['cleaned_lemma'] = [text_preprocessing(doc, True) for doc in train.text]\n",
    "valid['cleaned_lemma'] = [text_preprocessing(str(doc), True) for doc in valid.text]\n",
    "\n",
    "train['cleaned_stem'] = [text_preprocessing(doc, False) for doc in train.text]\n",
    "valid['cleaned_stem'] = [text_preprocessing(str(doc), False) for doc in valid.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "98e5450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features = 2000)\n",
    "\n",
    "X_train_lemma = vectorizer_tfidf.fit_transform(train.cleaned_lemma)\n",
    "X_valid_lemma = vectorizer_tfidf.transform(valid.cleaned_lemma)\n",
    "\n",
    "X_train_stem = vectorizer_tfidf.fit_transform(train.cleaned_stem)\n",
    "X_valid_stem = vectorizer_tfidf.transform(valid.cleaned_stem)\n",
    "\n",
    "\n",
    "# Balancing the data\n",
    "X_train_lemma_SMOTE, y_train_lemma_SMOTE = SMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_ROS, y_train_lemma_ROS = RandomOverSampler(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_ADASYN, y_train_lemma_ADASYN = ADASYN(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_BorderlineSMOTE, y_train_lemma_BorderlineSMOTE = BorderlineSMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE = SVMSMOTE(random_state = 42).fit_resample(X_train_lemma, train.label)\n",
    "\n",
    "# Balancing the data\n",
    "X_train_stem_SMOTE, y_train_stem_SMOTE = SMOTE(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_ROS, y_train_stem_ROS = RandomOverSampler(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_ADASYN, y_train_stem_ADASYN = ADASYN(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_BorderlineSMOTE, y_train_stem_BorderlineSMOTE = BorderlineSMOTE(random_state = 42).fit_resample(X_train_stem, train.label)\n",
    "X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE = SVMSMOTE(random_state = 42).fit_resample(X_train_stem, train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c2259",
   "metadata": {},
   "source": [
    "### Deep Learning model for dataset unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "51ce205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "    \n",
    "    return class_weight\n",
    "\n",
    "def get_f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "class_weights = create_class_weight({0: 7581, 1: 794})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b60a98",
   "metadata": {},
   "source": [
    "- X_train_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "819c2aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_345 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.3043 - get_f1: 4.4903e-04\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.2068 - get_f1: 0.2570\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.1405 - get_f1: 0.5953\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0511 - get_f1: 0.8635\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0089 - get_f1: 0.9365\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0023 - get_f1: 0.9561\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0011 - get_f1: 0.9576\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 9.8820e-04 - get_f1: 0.9682\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9725\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 9.8016e-04 - get_f1: 0.9420\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9611\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9348\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9652\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9576\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 9.8023e-04 - get_f1: 0.9725\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0013 - get_f1: 0.9382\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0012 - get_f1: 0.9460\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0012 - get_f1: 0.9567\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0013 - get_f1: 0.9573\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252fa74f610>"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma.toarray(), train.label, epochs = 20, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "fc8fae92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3058252427184466"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ade157",
   "metadata": {},
   "source": [
    "- X_train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "9f75420f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_233 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.2980 - get_f1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.2015 - get_f1: 0.3168\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.1319 - get_f1: 0.6225\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0588 - get_f1: 0.8207\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0181 - get_f1: 0.9032\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0055 - get_f1: 0.9598\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0018 - get_f1: 0.9758\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 1.7011e-04 - get_f1: 0.9275\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.9580e-05 - get_f1: 0.9542\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.6081e-05 - get_f1: 0.9618\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.0114e-05 - get_f1: 0.9618\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 6.9352e-06 - get_f1: 0.9504\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.0357e-06 - get_f1: 0.9733\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 3.8049e-06 - get_f1: 0.9733\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.9558e-06 - get_f1: 0.9542\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 2.3174e-06 - get_f1: 0.9542\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.8535e-06 - get_f1: 0.9695\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.4949e-06 - get_f1: 0.9733\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 1.2051e-06 - get_f1: 0.9542\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 9.7979e-07 - get_f1: 0.9656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d5987fa0>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem.toarray(), train.label, epochs = 20, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "b2e73605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2698412698412698"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb9272",
   "metadata": {},
   "source": [
    "### Deep Learning model for oversampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a58c0",
   "metadata": {},
   "source": [
    "- Lemmatized oversampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "0837d5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_238 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2213 - get_f1: 0.8744\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0149 - get_f1: 0.9952\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0013 - get_f1: 0.9998\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3557e-04 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.0467e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.1615e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.3752e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.8504e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8819e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4212e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0103e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4766e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.6045e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0714e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1316e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2888e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7318e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2923e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 9.7834e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4075e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d998b790>"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_SMOTE.toarray(), y_train_lemma_SMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d1cb68ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28228228228228225"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "656c64d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_243 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2703 - get_f1: 0.8890\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0227 - get_f1: 0.9932\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0010 - get_f1: 0.9997: 0s - loss: 0\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 3.0904e-05 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2834e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.5948e-06 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8720e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2838e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2853e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6244e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.1711e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.5271e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.2159e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.5284e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2901e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.4114e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7822e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3326e-07 - get_f1: 1.0000: 0s - loss: 1\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0098e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.7200e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d3ef9310>"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_ROS.toarray(), y_train_lemma_ROS, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "0de21a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21453287197231835"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "6d0a3874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_248 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "467/467 [==============================] - 2s 3ms/step - loss: 0.2107 - get_f1: 0.8803\n",
      "Epoch 2/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0131 - get_f1: 0.9963\n",
      "Epoch 3/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9986\n",
      "Epoch 4/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.3706e-05 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.0509e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.1418e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.2103e-06 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.2572e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.9953e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.1533e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.8387e-07 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 5.8356e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.3775e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 3.2648e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.6513e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.0140e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.6084e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 1.2424e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.8071e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.7881e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251e10bae50>"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_ADASYN.toarray(), y_train_lemma_ADASYN, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "e0e1e19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2774566473988439"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e46bf7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_253 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_255 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2269 - get_f1: 0.8959\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0347 - get_f1: 0.9906\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0064 - get_f1: 0.9981\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9989\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0040 - get_f1: 0.9989\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9987\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0021 - get_f1: 0.9989\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9989\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.2061e-04 - get_f1: 0.9997\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0242e-05 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8380e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.1941e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0844e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.7700e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.5533e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1077e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.0897e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3609e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.8228e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dd544160>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_BorderlineSMOTE.toarray(), y_train_lemma_BorderlineSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "becfff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19292604501607716"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a8986760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_258 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_260 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_261 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2325 - get_f1: 0.9048\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0527 - get_f1: 0.9849\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0169 - get_f1: 0.9948\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0051 - get_f1: 0.9983\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0039 - get_f1: 0.9987\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9988\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0029 - get_f1: 0.9990\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 9.7769e-04 - get_f1: 0.9999\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9989\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0025 - get_f1: 0.9989\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.5262e-05 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.4433e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.9319e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3916e-06 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0270e-06 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.6499e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.7165e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.2802e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.1943e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dc6b71c0>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma_SVMSMOTE.toarray(), y_train_lemma_SVMSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "d84bc6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3187660668380463"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1211070",
   "metadata": {},
   "source": [
    "- Stemmed oversampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "e3a1c719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_263 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2156 - get_f1: 0.9113\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0186 - get_f1: 0.9950\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0028 - get_f1: 0.9992\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0029 - get_f1: 0.9990\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.3316e-04 - get_f1: 0.9999\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9987\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0049 - get_f1: 0.9986\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0065 - get_f1: 0.9980\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0020 - get_f1: 0.9994\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.1098e-05 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5748e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.4686e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.6966e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.3555e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5435e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0374e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.2289e-08 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.1833e-08 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.8549e-08 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9383e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dc7c0970>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_SMOTE.toarray(), y_train_stem_SMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "73840663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28901734104046245"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "07c62209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_268 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2548 - get_f1: 0.8630\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0195 - get_f1: 0.9942\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0017 - get_f1: 0.9995\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4924e-04 - get_f1: 1.0000\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9705e-05 - get_f1: 1.0000\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7013e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0265e-05 - get_f1: 1.0000\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 6.9538e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8870e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.4511e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.5262e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8176e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3463e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0045e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4582e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.6091e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.3291e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2648e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.4811e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8974e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252e5efafa0>"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_ROS.toarray(), y_train_stem_ROS, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "6fb49daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2922077922077922"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "82f02f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_278 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2049 - get_f1: 0.8711\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0125 - get_f1: 0.9962\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0023 - get_f1: 0.9993\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0019 - get_f1: 0.9974\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.4966e-05 - get_f1: 0.9979\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2944e-05 - get_f1: 1.0000\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.4894e-06 - get_f1: 0.9979\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.8300e-06 - get_f1: 1.0000\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2742e-06 - get_f1: 1.0000\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2476e-06 - get_f1: 1.0000\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.4648e-06 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 8.9334e-07 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.7101e-07 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.9576e-07 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.9078e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.1927e-07 - get_f1: 0.9979\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6797e-07 - get_f1: 0.9979\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.2995e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0118e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.9387e-08 - get_f1: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251dd180910>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_ADASYN.toarray(), y_train_stem_ADASYN, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "96ce1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2821316614420063"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "ee0a5bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_283 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2203 - get_f1: 0.9126\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0347 - get_f1: 0.9898\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0087 - get_f1: 0.9974\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0036 - get_f1: 0.9990\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0319e-04 - get_f1: 0.9999\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0065 - get_f1: 0.9978\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0039 - get_f1: 0.9988\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0014 - get_f1: 0.9992\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0017 - get_f1: 0.9995\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.2841e-05 - get_f1: 1.0000\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.5652e-06 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.5436e-06 - get_f1: 1.0000: 0s - loss: 1.5365e\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.0679e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.6195e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 5.5385e-07 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.0730e-07 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.0208e-07 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2583e-07 - get_f1: 1.0000\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6969e-07 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d36577f0>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_BorderlineSMOTE.toarray(), y_train_stem_BorderlineSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "dfa8678a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28648648648648645"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e1612e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_288 (Dense)           (None, 32)                64032     \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_291 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,281\n",
      "Trainable params: 245,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "474/474 [==============================] - 2s 3ms/step - loss: 0.2141 - get_f1: 0.9009\n",
      "Epoch 2/20\n",
      "474/474 [==============================] - 1s 3ms/step - loss: 0.0443 - get_f1: 0.9873\n",
      "Epoch 3/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0135 - get_f1: 0.9957\n",
      "Epoch 4/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0044 - get_f1: 0.9985\n",
      "Epoch 5/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0041 - get_f1: 0.9987\n",
      "Epoch 6/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0010 - get_f1: 0.9996\n",
      "Epoch 7/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0067 - get_f1: 0.9974\n",
      "Epoch 8/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0035 - get_f1: 0.9990\n",
      "Epoch 9/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0030 - get_f1: 0.9992\n",
      "Epoch 10/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0012 - get_f1: 0.9996\n",
      "Epoch 11/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 0.0011 - get_f1: 0.9996\n",
      "Epoch 12/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.3890e-05 - get_f1: 1.0000\n",
      "Epoch 13/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.5751e-06 - get_f1: 1.0000\n",
      "Epoch 14/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.6556e-06 - get_f1: 1.0000\n",
      "Epoch 15/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2434e-07 - get_f1: 1.0000\n",
      "Epoch 16/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 7.8979e-08 - get_f1: 1.0000\n",
      "Epoch 17/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 4.9055e-08 - get_f1: 1.0000\n",
      "Epoch 18/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 3.5421e-08 - get_f1: 1.0000\n",
      "Epoch 19/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.7963e-08 - get_f1: 1.0000: 0s - loss: 2.862\n",
      "Epoch 20/20\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.3165e-08 - get_f1: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251e1b10580>"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 2000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_stem_SVMSMOTE.toarray(), y_train_stem_SVMSMOTE, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "2e76b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31666666666666665"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X_valid_stem.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d71c4",
   "metadata": {},
   "source": [
    "## Tokenizer and pad sequence, followed by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "94dfd579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 15s 52ms/step - loss: 0.3616 - accuracy: 0.9019 - get_f1: 4.4903e-04\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 14s 52ms/step - loss: 0.2661 - accuracy: 0.9079 - get_f1: 0.1214\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 14s 54ms/step - loss: 0.2012 - accuracy: 0.9280 - get_f1: 0.4351\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 14s 53ms/step - loss: 0.1600 - accuracy: 0.9442 - get_f1: 0.6029\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1263 - accuracy: 0.9587 - get_f1: 0.6941\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.0945 - accuracy: 0.9717 - get_f1: 0.7663\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.0659 - accuracy: 0.9820 - get_f1: 0.8634\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0493 - accuracy: 0.9883 - get_f1: 0.8826\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0406 - accuracy: 0.9915 - get_f1: 0.9153\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0312 - accuracy: 0.9944 - get_f1: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23170731707317074"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train.cleaned_lemma))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train.cleaned_lemma)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(valid.cleaned_lemma)\n",
    "\n",
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "rnn_tiny = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_tiny.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_tiny.add(layers.LSTM(128, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_tiny.add(layers.GlobalMaxPool1D()) \n",
    "rnn_tiny.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1(.0001) ))\n",
    "rnn_tiny.add(Dropout(0.1))\n",
    "rnn_tiny.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_tiny.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy', get_f1])\n",
    "history_tiny_rnn = rnn_tiny.fit(X_t, train.label, epochs=10)\n",
    "\n",
    "y = rnn_tiny.predict(X_te)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "4c218b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 17s 55ms/step - loss: 0.3548 - accuracy: 0.9052 - get_f1: 0.0000e+00\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.2482 - accuracy: 0.9151 - get_f1: 0.2248\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.1897 - accuracy: 0.9321 - get_f1: 0.4917\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.1457 - accuracy: 0.9503 - get_f1: 0.6494\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.1073 - accuracy: 0.9655 - get_f1: 0.7447\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.0811 - accuracy: 0.9764 - get_f1: 0.8309\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0552 - accuracy: 0.9869 - get_f1: 0.8799\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 16s 59ms/step - loss: 0.0411 - accuracy: 0.9904 - get_f1: 0.9122\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0306 - accuracy: 0.9946 - get_f1: 0.9233\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0296 - accuracy: 0.9940 - get_f1: 0.9193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2789473684210526"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train.cleaned_stem))\n",
    "#try texts_to_matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train.cleaned_stem)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(valid.cleaned_stem)\n",
    "\n",
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "rnn_tiny = Sequential()\n",
    "embedding_size = 128\n",
    "\n",
    "rnn_tiny.add(Embedding(max_features, embedding_size))\n",
    "#adding LSTM layer to help 'forget' then pooling\n",
    "rnn_tiny.add(layers.LSTM(128, return_sequences=True,name='lstm_layer'))        \n",
    "rnn_tiny.add(layers.GlobalMaxPool1D()) \n",
    "rnn_tiny.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1(.0001) ))\n",
    "rnn_tiny.add(Dropout(0.1))\n",
    "rnn_tiny.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_tiny.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy', get_f1])\n",
    "history_tiny_rnn = rnn_tiny.fit(X_t, train.label, epochs=10)\n",
    "\n",
    "y = rnn_tiny.predict(X_te)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f47c1",
   "metadata": {},
   "source": [
    "## Datasets applied on ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "70a58e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.5899960614415124\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.599647266313933\n",
      "F1 score for XGB on train is  0.6633081444164567\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.34076433121019106\n",
      "F1 score for Random Forest on validation is  0.057971014492753624\n",
      "F1 score for SVM on validation is  0.057971014492753624\n",
      "F1 score for XGB on validation is  0.15510204081632653\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.6011976047904192\n",
      "F1 score for Random Forest on train is  0.9993698802772526\n",
      "F1 score for SVM on train is  0.9808523780111179\n",
      "F1 score for XGB on train is  0.7047308319738989\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.35197368421052627\n",
      "F1 score for Random Forest on validation is  0.01990049751243781\n",
      "F1 score for SVM on validation is  0.2756183745583039\n",
      "F1 score for XGB on validation is  0.1721311475409836\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "rf = RandomForestClassifier(class_weight = 'balanced')\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma, train.label)\n",
    "rf.fit(X_train_lemma, train.label)\n",
    "svm.fit(X_train_lemma, train.label)\n",
    "xgb.fit(X_train_lemma, train.label)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma), train.label))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma), train.label))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC(class_weight = 'balanced')\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem, train.label)\n",
    "rf.fit(X_train_stem, train.label)\n",
    "svm.fit(X_train_stem, train.label)\n",
    "xgb.fit(X_train_stem, train.label)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem), train.label))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem), train.label))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "f887eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:36:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9276644863409004\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9978187586753916\n",
      "F1 score for XGB on train is  0.9716719626800081\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.34676007005253934\n",
      "F1 score for Random Forest on validation is  0.20610687022900764\n",
      "F1 score for SVM on validation is  0.10138248847926268\n",
      "F1 score for XGB on validation is  0.23225806451612904\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:37:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9316472545757072\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9930942895086322\n",
      "F1 score for XGB on train is  0.9713319810682893\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.350561797752809\n",
      "F1 score for Random Forest on validation is  0.15833333333333333\n",
      "F1 score for SVM on validation is  0.13392857142857142\n",
      "F1 score for XGB on validation is  0.23948220064724918\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "rf.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "svm.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "xgb.fit(X_train_lemma_SMOTE, y_train_lemma_SMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma_SMOTE), y_train_lemma_SMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "rf.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "svm.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "xgb.fit(X_train_lemma_SVMSMOTE, y_train_lemma_SVMSMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_lemma_SVMSMOTE), y_train_lemma_SVMSMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_lemma), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_lemma), valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "87662b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9310211946050096\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9982160555004956\n",
      "F1 score for XGB on train is  0.9723197407507427\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.38007380073800734\n",
      "F1 score for Random Forest on validation is  0.2007722007722008\n",
      "F1 score for SVM on validation is  0.13636363636363635\n",
      "F1 score for XGB on validation is  0.225705329153605\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:41:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "F1 score for Logistic Regression on train is  0.9418673705047839\n",
      "F1 score for Random Forest on train is  1.0\n",
      "F1 score for SVM on train is  0.9946290033817387\n",
      "F1 score for XGB on train is  0.9728598433702403\n",
      "\n",
      "F1 score for Logistic Regression on validation is  0.3744493392070484\n",
      "F1 score for Random Forest on validation is  0.12605042016806722\n",
      "F1 score for SVM on validation is  0.19742489270386268\n",
      "F1 score for XGB on validation is  0.2382445141065831\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized dataset\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "rf.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "svm.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "xgb.fit(X_train_stem_SMOTE, y_train_stem_SMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem_SMOTE), y_train_stem_SMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Stemmed dataset\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "rf.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "svm.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "xgb.fit(X_train_stem_SVMSMOTE, y_train_stem_SVMSMOTE)\n",
    "\n",
    "print(\"F1 score for Logistic Regression on train is \", f1_score(lr.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for Random Forest on train is \", f1_score(rf.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for SVM on train is \", f1_score(svm.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print(\"F1 score for XGB on train is \", f1_score(xgb.predict(X_train_stem_SVMSMOTE), y_train_stem_SVMSMOTE))\n",
    "print()\n",
    "print(\"F1 score for Logistic Regression on validation is \", f1_score(lr.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for Random Forest on validation is \", f1_score(rf.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for SVM on validation is \", f1_score(svm.predict(X_valid_stem), valid.label))\n",
    "print(\"F1 score for XGB on validation is \", f1_score(xgb.predict(X_valid_stem), valid.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656e5d9",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "0f06356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 11815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9185609, 10171650)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_lemma = [_text.split() for _text in train.cleaned_lemma]\n",
    "\n",
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size = 300, \n",
    "                                            window = 2, \n",
    "                                            min_count = 2)\n",
    "w2v_model.build_vocab(documents_lemma)\n",
    "words = w2v_model.wv.index_to_key\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)\n",
    "w2v_model.train(documents_lemma, total_examples=len(documents_lemma), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "d6a2cea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 23228\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.cleaned_lemma)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train.cleaned_lemma), maxlen = 300)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(valid.cleaned_lemma), maxlen = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "107620cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23228, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "3db5e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_37 (Embedding)    (None, 300, 300)          6968400   \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128)               219648    \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,188,177\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 6,968,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "262/262 [==============================] - 42s 154ms/step - loss: 0.2920 - get_f1: 0.0248\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 45s 171ms/step - loss: 0.2495 - get_f1: 0.1290\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 42s 159ms/step - loss: 0.2275 - get_f1: 0.2348\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 43s 162ms/step - loss: 0.1991 - get_f1: 0.3650\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.1639 - get_f1: 0.5181\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 43s 162ms/step - loss: 0.1172 - get_f1: 0.6559\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.0727 - get_f1: 0.7940\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 42s 158ms/step - loss: 0.0405 - get_f1: 0.8836\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 43s 164ms/step - loss: 0.0243 - get_f1: 0.9057\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 43s 163ms/step - loss: 0.0114 - get_f1: 0.9411\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 42s 160ms/step - loss: 0.0044 - get_f1: 0.9537\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 42s 162ms/step - loss: 0.0018 - get_f1: 0.9618\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 44s 169ms/step - loss: 8.4535e-04 - get_f1: 0.9542\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 44s 167ms/step - loss: 5.5656e-04 - get_f1: 0.9580\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 46s 175ms/step - loss: 3.9885e-04 - get_f1: 0.9542\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 43s 166ms/step - loss: 2.9976e-04 - get_f1: 0.9427\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 43s 163ms/step - loss: 2.2741e-04 - get_f1: 0.9389\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 46s 176ms/step - loss: 1.7929e-04 - get_f1: 0.9466\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 42s 162ms/step - loss: 1.4044e-04 - get_f1: 0.9580\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 42s 160ms/step - loss: 1.1318e-04 - get_f1: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23076923076923075"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[get_f1])\n",
    "history = model.fit(x_train, train.label,\n",
    "                    batch_size=32,\n",
    "                    epochs=20,\n",
    "                    verbose=1)\n",
    "\n",
    "y = model.predict(x_test)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6659ddb",
   "metadata": {},
   "source": [
    "### Self paced ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "b67cfc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43447332421340623\n",
      "0.3132036847492323\n"
     ]
    }
   ],
   "source": [
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=RandomForestClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train_lemma, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid_lemma)\n",
    "print(f1_score(clf.predict(X_train_lemma), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "46910237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41386499869689863\n",
      "0.30931796349663787\n"
     ]
    }
   ],
   "source": [
    "# Train an SPE classifier\n",
    "clf = SelfPacedEnsembleClassifier(\n",
    "        base_estimator=RandomForestClassifier(), \n",
    "        n_estimators=10,\n",
    "    ).fit(X_train_stem, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = clf.predict(X_valid_stem)\n",
    "print(f1_score(clf.predict(X_train_stem), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0595034",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "c01b3529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Class 1: 7581 - 6372\n"
     ]
    }
   ],
   "source": [
    "hate1 = pd.read_csv(\"hate1.csv\")\n",
    "hate1 = hate1[hate1.label == 1]\n",
    "hate1 = hate1.rename({'tweet': 'text'}, axis = 1)\n",
    "\n",
    "hate2 = pd.read_excel(\"hate2.xlsx\")\n",
    "hate2 = hate2[hate2.Label == 1]\n",
    "hate2 = hate2.rename({'Sentences': 'text', 'Label': 'label'}, axis = 1)\n",
    "\n",
    "hate3 = pd.read_csv('english_dataset.tsv',sep='\\t')\n",
    "hate3 = hate3[hate3['task_1'] == 'HOF']\n",
    "hate3 = hate3.rename({'task_1': 'label'}, axis = 1)\n",
    "\n",
    "hate4 = pd.read_csv('hasoc2019_en_test-2919.tsv', sep = '\\t')\n",
    "hate4 = hate4[hate4['task_1'] == 'HOF']\n",
    "hate4 = hate4.rename({'task_1': 'label'}, axis = 1)\n",
    "\n",
    "hate5 = pd.read_csv('hate5.csv', sep = ';')\n",
    "hate5.isHate = hate5.isHate.astype(int)\n",
    "hate5 = hate5[hate5['isHate'] == 1]\n",
    "hate5 = hate5.rename({'comment': 'text', 'isHate': 'label'}, axis = 1)\n",
    "\n",
    "train = pd.concat([train, hate1, hate2, hate3, hate4, hate5])\n",
    "train = train[['text', 'label']]\n",
    "train.label = train.label.map({0: 0, 1: 1, 'HOF': 1})\n",
    "print(\"Class 0 - Class 1: {} - {}\".format(len(train[train.label == 0]), len(train[train.label == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "56ece5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "      <td>scheme saw estimated child poor family sent pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "      <td>durban homeless community reconciliation lunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "      <td>next immediate problem cropped assist unfortun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "      <td>far important implication economy god dollar l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "      <td>strengthen child sensitive social protection s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>All muslims worship Satan¬ü¬ï¬ã 5 times a day</td>\n",
       "      <td>1</td>\n",
       "      <td>muslim worship satan time day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>all mental illnesses are awful and must be tre...</td>\n",
       "      <td>1</td>\n",
       "      <td>mental illness awful must treated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>All girls like you should have a pimp to guide...</td>\n",
       "      <td>1</td>\n",
       "      <td>girl like pimp guide successful prostitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>After all this time the african americans are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>time african american people crime rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>actually autism can make you a retard</td>\n",
       "      <td>1</td>\n",
       "      <td>actually autism make retard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13953 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "0    the scheme saw an estimated 150,000 children f...      1   \n",
       "1    durban 's homeless communities reconciliation ...      1   \n",
       "2    the next immediate problem that cropped up was...      1   \n",
       "3    far more important than the implications for t...      1   \n",
       "4    to strengthen child-sensitive social protectio...      1   \n",
       "..                                                 ...    ...   \n",
       "158      All muslims worship Satan¬ü¬ï¬ã 5 times a day      1   \n",
       "159  all mental illnesses are awful and must be tre...      1   \n",
       "160  All girls like you should have a pimp to guide...      1   \n",
       "161  After all this time the african americans are ...      1   \n",
       "162              actually autism can make you a retard      1   \n",
       "\n",
       "                                         cleaned_lemma  \n",
       "0    scheme saw estimated child poor family sent pa...  \n",
       "1       durban homeless community reconciliation lunch  \n",
       "2    next immediate problem cropped assist unfortun...  \n",
       "3    far important implication economy god dollar l...  \n",
       "4    strengthen child sensitive social protection s...  \n",
       "..                                                 ...  \n",
       "158                      muslim worship satan time day  \n",
       "159                  mental illness awful must treated  \n",
       "160       girl like pimp guide successful prostitution  \n",
       "161            time african american people crime rate  \n",
       "162                        actually autism make retard  \n",
       "\n",
       "[13953 rows x 3 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_lemma'] = [text_preprocessing(doc, True) for doc in train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "da9b154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352062357908413\n",
      "0.23003194888178913\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features = 5000)\n",
    "\n",
    "X_train_lemma = vectorizer_tfidf.fit_transform(train.cleaned_lemma)\n",
    "X_valid_lemma = vectorizer_tfidf.transform(valid.cleaned_lemma)\n",
    "\n",
    "# Train an SPE classifier\n",
    "lr2 = LogisticRegression(class_weight = 'balanced')\n",
    "lr2.fit(X_train_lemma, train.label)\n",
    "\n",
    "# Predict with an SPE classifier\n",
    "y_pred = lr2.predict(X_valid_lemma)\n",
    "print(f1_score(lr2.predict(X_train_lemma), train.label))\n",
    "print(f1_score(y_pred, valid.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "39f46c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = create_class_weight({0: 7581, 1: 6372})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "5a568a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_319 (Dense)           (None, 32)                160032    \n",
      "                                                                 \n",
      " dense_320 (Dense)           (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 341,281\n",
      "Trainable params: 341,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "437/437 [==============================] - 3s 4ms/step - loss: 0.2777 - get_f1: 0.8602\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.1306 - get_f1: 0.9442\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0576 - get_f1: 0.9740\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0227 - get_f1: 0.9876\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0095 - get_f1: 0.9969\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0045 - get_f1: 0.9965\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0012 - get_f1: 0.9997\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0017 - get_f1: 0.9973\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 7.9105e-04 - get_f1: 0.9998\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 2s 5ms/step - loss: 0.0016 - get_f1: 0.9995\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 6.6591e-04 - get_f1: 0.9976\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0028 - get_f1: 0.9991\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0070 - get_f1: 0.9951\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 0.0048 - get_f1: 0.9980\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 9.9921e-04 - get_f1: 0.9975\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 3.3615e-04 - get_f1: 0.9999\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.6653e-04 - get_f1: 0.9999\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.4647e-04 - get_f1: 0.9976\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.1354e-04 - get_f1: 0.9999\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 2s 4ms/step - loss: 2.0457e-04 - get_f1: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21553884711779447"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "Dense(input_dim = 5000, units = 32, activation = 'relu'),\n",
    "Dense(units = 512, activation = 'relu'),\n",
    "Dense(units = 256, activation = 'relu'),\n",
    "Dense(units = 128, activation = 'relu'),\n",
    "Dense(units = 1, activation = 'sigmoid'),])\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [get_f1])\n",
    "model.fit(X_train_lemma.toarray(), train.label, epochs = 20, class_weight = class_weights)\n",
    "y = model.predict(X_valid_lemma.toarray())\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "28db5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], output_shape=[512,16], dtype=tf.string,trainable= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "4cc599be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "437/437 [==============================] - 840s 2s/step - loss: 0.3187 - get_f1: 0.8716\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.2036 - get_f1: 0.9519\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 833s 2s/step - loss: 0.1016 - get_f1: 0.9842\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0680 - get_f1: 0.9907\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 834s 2s/step - loss: 0.0838 - get_f1: 0.9892\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0564 - get_f1: 0.9931\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0552 - get_f1: 0.9936\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0551 - get_f1: 0.9935\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 833s 2s/step - loss: 0.0551 - get_f1: 0.9937\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0551 - get_f1: 0.9961\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0551 - get_f1: 0.9938\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9936\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9936\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0551 - get_f1: 0.9958\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9957\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9960\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 832s 2s/step - loss: 0.0551 - get_f1: 0.9958\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9960\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9938\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 831s 2s/step - loss: 0.0551 - get_f1: 0.9960\n",
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_input'), name='keras_layer_input', description=\"created by layer 'keras_layer_input'\"), but it was called on an input with incompatible shape (None, 5000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_input'), name='keras_layer_input', description=\"created by layer 'keras_layer_input'\"), but it was called on an input with incompatible shape (None, 5000).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"keras_layer\" (type KerasLayer).\n    \n    in user code:\n    \n        File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,5000], [].\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 5000), dtype=string)\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1248/2085697370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleaned_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_lemma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"keras_layer\" (type KerasLayer).\n    \n    in user code:\n    \n        File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,5000], [].\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 5000), dtype=string)\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "def get_model(dense_1 = 128, dense_2 = 64):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        hub_layer,\n",
    "        tf.keras.layers.Dense(dense_1, activation='relu'),\n",
    "        tf.keras.layers.Dense(dense_2, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Train the model for a specified number of epochs.\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[get_f1])\n",
    "\n",
    "model.fit(x = train.cleaned_lemma, y = train.label, epochs = 20, class_weight = class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "fdc6712d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23163841807909605"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(valid.cleaned_lemma)\n",
    "y = y.flatten()\n",
    "y_pred = np.where(y > 0.5, 1, 0)\n",
    "f1_score(y_pred, valid.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45065f4",
   "metadata": {},
   "source": [
    "### Other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eeaab6",
   "metadata": {},
   "source": [
    "### BERT Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "17bfc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "457238d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8375/8375 [00:11<00:00, 753.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2094/2094 [00:02<00:00, 763.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "# For every sentence...\n",
    "for sentence in tqdm(train.text.values):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "train_labels = torch.tensor(train.label.values)\n",
    "\n",
    "\n",
    "########## Validation\n",
    "\n",
    "input_ids_valid = []\n",
    "attention_masks_valid = []\n",
    "\n",
    "# For every sentence...\n",
    "for sentence in tqdm(valid.text.astype(str).values):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_valid.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_valid.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_valid = torch.cat(input_ids_valid, dim=0)\n",
    "attention_masks_valid = torch.cat(attention_masks_valid, dim=0)\n",
    "valid_labels = torch.tensor(valid.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "fa25a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine the training and validation inputs into TensorDataset\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, train_labels)\n",
    "valid_dataset = TensorDataset(input_ids_valid, attention_masks_valid, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "c96d5d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 8375/8375 [00:00<00:00, 115813.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7581  794]\n",
      "tensor([0.0013, 0.0013, 0.0013,  ..., 0.0001, 0.0001, 0.0001],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "label = []\n",
    "\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    label.append(train_dataset[i][2].item())\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(label == t)[0]) for t in np.unique(label)])\n",
    "print(class_sample_count)\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in label])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "print(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = sampler, \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "d46dc973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import time, random, datetime, torch\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "4e28f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    262.    Elapsed: 0:04:50.\n",
      "  Batch    80  of    262.    Elapsed: 0:09:44.\n",
      "  Batch   120  of    262.    Elapsed: 0:14:32.\n",
      "  Batch   160  of    262.    Elapsed: 0:19:20.\n",
      "  Batch   200  of    262.    Elapsed: 0:24:12.\n",
      "  Batch   240  of    262.    Elapsed: 0:29:52.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 0:32:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 0:02:58\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    262.    Elapsed: 0:05:31.\n",
      "  Batch    80  of    262.    Elapsed: 0:10:56.\n",
      "  Batch   120  of    262.    Elapsed: 0:16:22.\n",
      "  Batch   160  of    262.    Elapsed: 0:21:44.\n",
      "  Batch   200  of    262.    Elapsed: 0:27:08.\n",
      "  Batch   240  of    262.    Elapsed: 0:32:27.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epcoh took: 0:35:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:02:54\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    262.    Elapsed: 0:05:29.\n",
      "  Batch    80  of    262.    Elapsed: 0:10:43.\n",
      "  Batch   120  of    262.    Elapsed: 0:15:54.\n",
      "  Batch   160  of    262.    Elapsed: 0:20:55.\n",
      "  Batch   200  of    262.    Elapsed: 0:26:07.\n",
      "  Batch   240  of    262.    Elapsed: 0:31:14.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:33:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation Loss: 0.37\n",
      "  Validation took: 0:02:48\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:50:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        model_output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "            (loss, logits) = model_output.loss, model_output.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "7a9c135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 8,375 test sentences...\n",
      "    DONE.\n",
      "F1 score for validation:  0.5074626865671642\n"
     ]
    }
   ],
   "source": [
    "# Prediction on validation set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in validation_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "predictions = np.array([1 if x[0] < 0.5 else 0 for x in np.concatenate(predictions, axis=0)])\n",
    "print(\"F1 score for validation: \", f1_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "fe963a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(paragraph):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for sentence in tqdm([paragraph]):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  \n",
    "  dataset = TensorDataset(input_ids, attention_masks)\n",
    "  dataloader = DataLoader(\n",
    "          dataset,\n",
    "          batch_size = 1\n",
    "      )\n",
    "  \n",
    "  for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "    predictions = np.array([1 if x[0] < 0.5 else 0 for x in np.concatenate([logits], axis=0)])\n",
    "    return predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd9299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
