\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{eurosym}
\usepackage[final]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{multirow}




\title{University of Bucharest Team at Semeval-2022 Task4: Detection and
	Classification of Patronizing and Condescending Language}

\author{Raluca Andreea G\^inga, {\bf Bogdan Dobre},  \\  {\bf Tudor-Andrei Dumitra\c{s}cu} \and {\bf Bogdan Radu Silviu Sielecki}\\
University of Bucharest \\
\texttt{\{gingaraluca, dobrebogdan98, }\\
\texttt{tudorandrei.dumitrascu, sieleckiradu\}@gmail.com }
}


% TODO: These need to be fixed.
% Please add proper citations to all models used (Word2Vec should be Mikolov et al., 2013 - also please specify if you used CBOW or SkipGram-, LSTMs should be Hochreiter, S., & Schmidhuber 1997, etc), and software (provide link and/or citation for the transformers library*)
% Please add more details on the speech dataset you collected and the motivation behind this.
% Please add captions for results tables, and at least a brief discussion of the results. Given that in this paper there is a significant amount of work, results and systems compared, it would benefit the authors to provide a discussion section so other readers can learn from this work.

\begin{document}
\maketitle

\begin{abstract}
	This paper details our implementations for finding Patronizing and
	Condescending Language in texts, as part of the SemEval Workshop Task 4.
	We have used a variety of methods from simple machine learning algorithms applied
	on bag of words, all the way to BERT models, in order to solve the binary classification
	and the multi-label multi-class classification.
\end{abstract}

\section{Introduction}

The Patronizing and Condescending Language Detection Task \cite{perezalmendros2022semeval} is based on the
paper Don't Patronize Me! \cite{perezalmendros2020dont}, which is an annotated Dataset with Patronizing and
Condescending Language Towards Vulnerable Communities.

The aim of this task is to identify PCL, and to categorize the language
used to express it, specifically when referring to communities
identified as being vulnerable to unfair treatment in the media.

Participants were provided with sentences in context (paragraphs), extracted
from news articles, in which one or several predefined vulnerable
communities are mentioned. The challenge is divided into two subtasks.

\begin{enumerate}
	\item Subtask 1: Binary classification. Given a paragraph, a system must
	      predict whether or not it contains any form of PCL.

	\item Subtask 2: Given a paragraph, a system must identify which PCL
	      categories express the condescension. The PCL taxonomy was defined
	      based on previous works on PCL (i.g. Unbalanced power relations, Shallow
	      solution, Presupposition, Authority voice, Metaphor, Compassion, The poorer,
	      the merrier. )
\end{enumerate}


\section{Background}

The dataset used for this SemEval 2022 task was Don't Patronize Me! \cite{perezalmendros2020dont},
which contains a suite of sentences that mention some vulnerable communities
and published in media in a lot of English speaking countries. The
paragraphs were manually annotated to show 1) whether the text contains any
kind of PCL, and 2) if it contains PCL, what linguistic techniques
(categories) are used to express the condescension.

The dataset for subtask 1 (binary classification) contained a number of
10.636 paragraphs and 2.792 instances were used for the categories
classification subtask.

In Figure \ref{fig1}, it can be seen that for the first subtask, there are almost
1000 texts that contain PCL. This means that the dataset is highly imbalanced and needs to be addressed.
The next three figures (\ref{fig2},\ref{fig3},\ref{fig4}) display the distribution
of the most common words, both for the full dataset and for those texts that contain
/ do not contain PCL.

% TODO: Remove the photos if we're not mentioning or discussing them

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{DataDistribution.png}
	\caption{Classes Distribution for Binary Classification problem (Subtask 1)}
	\label{fig1}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{common.png}
	\caption{Most common words in the dataset (Subtask 1)}
	\label{fig2}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{pcl.png}
	\caption{Most common words classified into PCL}
	\label{fig3}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{nopcl.png}
	\caption{Most common words of texts that are not PCL}
	\label{fig4}
\end{figure}


For task 2, the paragraphs from task 1 are split according to the type of PCL speech category into sentences, resulting in 950 samples.

\section{System Overview}

\begin{enumerate}

	\item Subtask 1 (Binary Classification)

	      Because the dataset was very imbalanced, we tried different approaches in
	      order to make it balanced:

	      \begin{itemize}
		      \item Adding a class weight to the models used. In this approach, we
		            computed a metric in which we obtained a class weight according to the
		            imbalance of the dataset. Through this method, we gave some different
		            weights to both the majority and minority classes. This whole process had
		            the purpose to penalize the miss classification made by the minority class by
		            setting a higher class weight and at the same time, reducing the weight for
		            the majority class.

		      \item Using oversampling methods and special ensemble techniques. In this
		            approach, we used methods like SMOTE (Synthetic Minority
		            Over-sampling Technique), Adasyn (Adaptive Synthetic), SVM-SMOTE and self
		            paced ensemble that performs strictly balanced under-sampling in each
		            iteration, being very efficient computationally.

		      \item Augmenting the data. Because we notice so little data for label 1, we
		            decided to collect hate speech datasets and add the positive texts into our
		            dataset in order to balance the classes frequency, obtaining a total of 6372
		            from 795 initial texts with label 1. We will notice in the results section
		            that this collection and generation of new dataset did not provide good
		            results.
	      \end{itemize}

	      The dataset was a little bit preprocessed and split into two preprocessed
	      types: lemmatized cleaned dataset and stemmed cleaned dataset. These two
	      datasets were generated in order to make some comparison between those two
	      techniques and to see which provided the best results.

	      To extract features from text, we have used Bag of
	      Words, Tokenizer, Word2Vec and, finally, BertTokenizer.

	      We have also used a variety of models such as Neural Networks with 3 dense layers, Long Short
	      Term Memory (LSTM) with 64 and 128 neurons with dropout as well, basic
	      Machine Learning algorithms like Logistic Regression, Random Forest, Support
	      Vector Machines as XGBoost. In the end, we decided to try BERT embeddings
	      and a BERT classification model, BertForSequenceClassification, that
	      contains a single linear classification layer on top and that provided the
	      best results after all of the other approaches.

	      Another approach, called "Text shards" made use of the subtask related to
	      multi-class classification as well. For an average text that contains PCL,
	      only some small pieces of them are actually PCL and the rest of the text are
	      not. The assumption is that this confuses the model, because a combination
	      of PCL and non-PCL is labeled as PCL. To address this, the following
	      approach is used:

	      \begin{itemize}
		      \item negative examples are left as they are

		      \item each positive example is replaced with the actual pieces of PCL inside
		            it that we can get from the categories file

		      \item the positive examples obtained this way are added with the negative
		            examples to obtain a training dataset

		      \item all the sentences are cleaned of characters that are not letters and
		            the words in each sentence are lemmatized

		      \item a Tensorflow Hub pretrained model called Universal Sentence Encoder is
		            trained on it

		      \item for each text that we want to predict, we first use the model on the
		            whole text to get an initial label

		      \item a window (of the size of the average length of a cleaned PCL fragment
		            * 2) is slided through the text and the model is used to predict that
		            particular substring. If it is labeled as PCL, then we consider the whole
		            text as PCL.
	      \end{itemize}

	\item Subtask 2 (Multi-label Classification)

	      Considering the fact that the vocabulary of the is English is large,
	      we have tried to leverage the power of pretrained language models.
	      Therefore we have chosen 3 BERT-based models which were pretrained
	      for hate speech detection and sentiment analysis. The BERT models
	      also provided a tokenizer which split the sentences into tokens and
	      appended the required tokens.

	      \begin{itemize}
		      \item BERT \cite{bert} Uncased

		      \item  BERT Multilingual Uncased

		      \item BERT HateXplain \cite{mathew2020hatexplain}: This model was trained to classify text as Hate speech, Offensive or Normal.
		            It was trained on Gab, Twitter and Humain Rationale;

		      \item Distil BERT : This model is a version of Distilled BERT finetuned on the Twitter dataset;

		      \item Distil BERT Multilingual Cased \cite{DistilBERT}

		      \item Distill RoBERTa : This model is a version of Distilled RoBERTa finetuned on the Twitter dataset;
	      \end{itemize}

	      In the paper describing the dataset \cite{perezalmendros2020dont},
	      the authors group the categories into 3 General categories.

	      \begin{enumerate}
		      \item The saviour: Unbalanced power relations and Shallow relations

		      \item  The Expert: Presupposition and Authority voice

		      \item The Poet: Compassion, Metaphor and The poorer the merrier

	      \end{enumerate}

	      From this idea, we tried to train the models to predict those 3
	      categories, and save the hidden features to a fixed latent space.
	      Then these learned features can be used when training the model to
	      predict the required 7 sub-classes.

	      Along with those BERT-based model, we also tried to implement models based on Word2Vec\cite{goldberg2014word2vec}
	      trained on "Google News" and Machine Learning algorithms based on TF-IDF and BOW:

	      \begin{itemize}
		      \item LSTM Word2Vec Embeddings \cite{staudemeyer2019understanding}
		      \item biLSTM Word2Vec Embeddings \cite{huang2015bidirectional}
		      \item RNN Word2Vec Embeddings \cite{rnn}
		      \item SVM TF-IDF
		      \item RandomForest TF-IDF
	      \end{itemize}



          We also dabbled with the thought of training our own Word2Vec, in
          order to create a model specialized on hate speech. However we
          decided against this idea, due to the lack of usable datasets and the
          computational resources required for this task.


\end{enumerate}


\section{Results}

\begin{enumerate}

	% TODO: Re Write the tables
	\item Subtask 1 (Binary Classification)

	      \begin{enumerate}
		      \item Deep Learning / Machine Learning for Imbalanced and Oversampled
		            dataset

		            \scalebox{0.5}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                                    & \multicolumn{4}{|c|}{Lemmatized (F1\_Score)}                            \\ \hline
				            Approach                & Simple                                       & SPE  & SMOTE  & SVMSMOTE
				            \\ \hline
				            Neural Networks         & 0.27                                         & -    & 0.2823 & 0.3187
				            \\ \hline
				            Logistic Regression     & 0.34                                         & -    & 0.35   & 0.35     \\
				            \hline
				            Random Forest           & 0.067                                        & 0.31 & 0.19   & 0.16     \\
				            \hline
				            Support Vector Machines & 0.27                                         & -    & 0.10   & 0.14     \\
				            \hline
				            XGBoost                 & 0.15                                         & -    & 0.23   & 0.24     \\ \hline
			            \end{tabular}
		            }

		            \scalebox{0.5}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                                    & \multicolumn{4}{|c|}{Stemmed (F1\_Score)}                           \\ \hline
				            Approach                & Simple                                    & SPE  & SMOTE & SVMSMOTE
				            \\ \hline
				            Neural Networks         & 0.2698                                    & -    & 0.289 & 0.3166
				            \\ \hline
				            Logistic Regression     & 0.35                                      & -    & 0.38  & 0.37     \\
				            \hline
				            Random Forest           & 0.038                                     & 0.31 & 0.21  & 0.13     \\
				            \hline
				            Support Vector Machines & 0.27                                      & -    & 0.14  & 0.20     \\
				            \hline
				            XGBoost                 & 0.17                                      & -    & 0.23  & 0.24     \\ \hline
			            \end{tabular}
		            }

		      \item Tokenization of the lemmatized and stemmed dataset / Word2Vec + LSTM
		            neural network

		            \scalebox{0.5}{
			            \begin{tabular}[t]{|l|l|l|l|l|}
				            \hline
				                               & \multicolumn{2}{|l}{Lemmatized (F1\_Score)} & \multicolumn{2}{|l|}{Stemmed
				            (F1\_Score)}                                                                                                        \\ \hline
				            Approach           & Simple                                      & Word2Vec                     & Simple & Word2Vec \\ \hline
				            LSTM (64 neurons)  & 0.2693                                      & 0.2109                       & 0.3213 & 0.2093   \\ \hline
				            LSTM (128 neurons) & 0.2317                                      & 0.2308                       & 0.2789 & 0.2412   \\ \hline
			            \end{tabular}
		            }

		      \item Data augmentation

		            \scalebox{0.5}{
			            \begin{tabular}[t]{|l|l|}
				            \hline
				                                    & Augmented Data (F1\_Score) \\ \hline
				            NNs with 3 layers       & 0.2155                     \\ \hline
				            Logistic Regression     & 0.23                       \\ \hline
				            U.S.E. + 2 dense layers & 0.2316                     \\ \hline
			            \end{tabular}
		            }

		      \item BERT Transformers + BertForSequenceClassification

		            \scalebox{0.55}{
			            \begin{tabular}[t]{|l|l|}
				            \hline
				                                        & Unprocessed data (F1\_Score) \\ \hline
				            Bert Tokenizer + Classifier & 0.5074                       \\ \hline
			            \end{tabular}
		            }

		      \item Text shards

		            \scalebox{0.55}{
			            \begin{tabular}[t]{|l|l|}
				            \hline
				                        & Lemmas F1\_Score \\
				                        &                  \\ \hline
				            Text shards & 0.3117           \\ \hline
			            \end{tabular}
		            }
	      \end{enumerate}

	\item Subtask 2 (Multi-class Classification)

	      \begin{enumerate}
              % TODO:  rephrase
		      \item BERT models approach for classification across 7 classes. In the table it
		            can be seen that the models can easily two of the categories,
		            while the rest of them are not learned

		            \scalebox{0.7}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                          & \multicolumn{4}{|c|}{F1 Score}                   \\ \hline
				            Class         & Unb                            & Sha & Pre & Aut
				            \\ \hline
				            BERT          & 0.82                           & 0.0 & 0.0 & 0.0
				            \\ \hline
				            DistilRoBERTa & 0.83                           & 0.0 & 0.0 & 0.0 \\
				            \hline
				            DistilBERT    & 0.82                           & 0.0 & 0.0 & 0.0 \\
				            \hline
			            \end{tabular}
		            }
		            \scalebox{0.7}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                          & \multicolumn{4}{|c|}{F1 Score}                            \\ \hline
				            Class         & Met                            & Com  & Mer  & Mean Score
				            \\ \hline
				            BERT          & 0.0                            & 0.0  & 0.64 & 0.21
				            \\ \hline
				            DistilRoBERTa & 0.0                            & 0.0  & 0.59 & 0.20       \\
				            \hline
				            DistilBERT    & 0.66                           & 0.08 & 0.0  & 0.34       \\
				            \hline
			            \end{tabular}
		            }
		      \item The general class approach is detailed in the following tables.
		            It shows that the general classes were learned, but when using the
		            pretrained models and fine-tuning on the specific classes, some
		            of previously learned features are lost.

		            \scalebox{0.8}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                             & \multicolumn{3}{|c|}{F1 Score}              \\
				            \hline
				            Main/Sub-classes & Expert                         & Aut & Pre  \\
				            \hline
				            BERT             & 0.44                           & 0.0 & 0.0  \\
				            \hline
				            DistilRoBERTa    & 0.54                           & 0.0 & 0.0  \\
				            \hline
				            DistilBERT       & 0.42                           & 0.0 & 0.40 \\
				            \hline
				            DistilBERTMLC    & 0.36                           & 0.0 & 0.0  \\
				            \hline
			            \end{tabular}
		            }

		            \scalebox{0.7}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                             & \multicolumn{3}{|c|}{F1 Score}              \\
				            \hline
				            Main/Sub-classes & Saviour                        & Sha & Unb  \\
				            \hline
				            BERT             & 0.85                           & 0.0 & 0.84 \\
				            \hline
				            DistilRoBERTa    & 0.85                           & 0.0 & 0.84 \\
				            \hline
				            DistilBERT       & 0.75                           & 0.0 & 0.81 \\
				            \hline
				            DistilBERTMLC    & 0.86                           & 0.0 & 0.83 \\
				            \hline
			            \end{tabular}
		            }

		            \scalebox{0.7}{
			            \begin{tabular}[t]{|l|l|l|l|l|l|l|l|l|}
				            \hline
				                             & \multicolumn{4}{|c|}{F1 Score}                     \\
				            \hline
				            Main/Sub-classes & Poet                           & Com  & Mer & Met  \\
				            \hline
				            BERT             & 0.69                           & 0.0  & 0.0 & 0.59 \\
				            \hline
				            DistilRoBERTa    & 0.69                           & 0.11 & 0.0 & 0.65 \\
				            \hline
				            DistilBERT       & 0.61                           & 0.0  & 0.0 & 0.67 \\
				            \hline
				            DistilBERTMLC    & 0.60                           & 0.0  & 0.0 & 0.52 \\
				            \hline
			            \end{tabular}
		            }
	      \end{enumerate}
\end{enumerate}


\section{Conclusion}

% TODO: Draw conclusions
In the current project, it was solved the problem posed by SemEval 2022 Task
4: Patronizing and Condescending Language Detection. There were applied
various methods, including the application of Word Embedings (Bag of Words,
Word2Vec, BERT), tokenization, oversampling/undersampling of the datasets.

In the binary classification problem, the approach that gave the best result
on the validation dataset was BERT transformers combined with BERT for
Sequence Classification, obtaining 0.50 as f1\_score.

In the multi classification multi label task, the number of labels proved to
be a challenge. The results overall are low and the models are only able to learn
only a few classes. The general class approach also proved to be inefficient.

Some recommendations for future work could be to have a better approach and introduce
more linguistic insight in the approach.

\section*{Acknowledgements}

We would like to thank our supervisor Ana-Sabina Uban for the guidance and throughout
the development of the project.



\bibliography{custom}

% TODO: Appendix with hparams and hparams search maybe?
\end{document}
