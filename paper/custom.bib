% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@Article{mathew2020hatexplain,
  title        = "HateXplain: {A} Benchmark Dataset for Explainable Hate
                 Speech Detection",
  author       = "Binny Mathew and Punyajoy Saha and Seid Muhie Yimam
                 and Chris Biemann and Pawan Goyal and Animesh
                 Mukherjee",
  journal      = "arXiv preprint arXiv:2012.10289",
  year         = "2020",
}

@Article{Ando2005,
  acmid        = "1194905",
  author       = "Rie Kubota Ando and Tong Zhang",
  ISSN         = "1532-4435",
  issue_date   = "12/1/2005",
  journal      = "Journal of Machine Learning Research",
  month        = dec,
  numpages     = "37",
  pages        = "1817--1853",
  publisher    = "JMLR.org",
  title        = "A Framework for Learning Predictive Structures from
                 Multiple Tasks and Unlabeled Data",
  volume       = "6",
  year         = "2005",
}

@InProceedings{perezalmendros2020dont,
  title        = "{Don’t Patronize Me! An Annotated Dataset with
                 Patronizing and Condescending Language towards
                 Vulnerable Communities}",
  author       = "Carla P{'e}rez-Almendros and Luis Espinosa-Anke and
                 Steven Schockaert",
  booktitle    = "Proceedings of the 28th International Conference on
                 Computational Linguistics",
  pages        = "5891--5902",
  year         = "2020",
}

@Misc{staudemeyer2019understanding,
  title        = "Understanding {LSTM} -- a tutorial into Long
                 Short-Term Memory Recurrent Neural Networks",
  author       = "Ralf C. Staudemeyer and Eric Rothstein Morris",
  year         = "2019",
  eprint       = "1909.09586",
  archiveprefix = "arXiv",
  primaryclass = "cs.NE",
}

@Misc{huang2015bidirectional,
  title        = "Bidirectional {LSTM}-{CRF} Models for Sequence
                 Tagging",
  author       = "Zhiheng Huang and Wei Xu and Kai Yu",
  year         = "2015",
  eprint       = "1508.01991",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
}

@Article{rnn,
  title        = "Fundamentals of Recurrent Neural Network ({RNN}) and
                 Long Short-Term Memory ({LSTM}) network",
  volume       = "404",
  ISSN         = "0167-2789",
  URL          = "http://dx.doi.org/10.1016/j.physd.2019.132306",
  doi          = "10.1016/j.physd.2019.132306",
  journal      = "Physica D: Nonlinear Phenomena",
  publisher    = "Elsevier BV",
  author       = "Alex Sherstinsky",
  year         = "2020",
  month        = mar,
  pages        = "132306",
}

@Article{SentimentClassification,
  author       = "Saurav Singla and Vikash Kumar",
  year         = "2020",
  month        = "11",
  pages        = "14--20",
  title        = "Multi-Class Sentiment Classification using Machine
                 Learning and Deep Learning Techniques",
  volume       = "8",
  journal      = "INTERNATIONAL JOURNAL OF COMPUTER SCIENCES AND
                 ENGINEERING",
  doi          = "10.26438/ijcse/v8i11.1420",
}

@InProceedings{7511392,
  author       = "Mondher Bouazizi and Tomoaki Ohtsuki",
  booktitle    = "2016 IEEE International Conference on Communications
                 (ICC)",
  title        = "Sentiment analysis: From binary to multi-class
                 classification: {A} pattern-based approach for
                 multi-class sentiment analysis in Twitter",
  year         = "2016",
  pages        = "1--6",
  doi          = "10.1109/ICC.2016.7511392",
}

@Article{article,
  author       = "Muhammed Enes Atik and Z. Duran",
  year         = "2021",
  month        = "04",
  pages        = "4270--4275",
  title        = "Classification of Aerial Photogrammetric Point Cloud
                 Using Recurrent Neural Networks",
  volume       = "30",
  journal      = "Fresenius Environmental Bulletin",
}

@Misc{lopez2017deep,
  title        = "Deep Learning applied to {NLP}",
  author       = "Marc Moreno Lopez and Jugal Kalita",
  year         = "2017",
  eprint       = "1703.03091",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
}

@Article{cmc.2021.017827,
  author       = "Zhanl Mamykova {Galimkair Mutanov, Vladislav
                 Karyukin}",
  title        = "Multi-Class Sentiment Analysis of Social Media Data
                 with Machine Learning Algorithms",
  journal      = "Computers, Materials \& Continua",
  volume       = "69",
  year         = "2021",
  number       = "1",
  pages        = "913--930",
  URL          = "http://www.techscience.com/cmc/v69n1/42767",
  ISSN         = "1546-2226",
  abstract     = "The volume of social media data on the Internet is
                 constantly growing. This has created a substantial
                 research field for data analysts. The diversity of
                 articles, posts, and comments on news websites and
                 social networks astonishes imagination. Nevertheless,
                 most researchers focus on posts on Twitter that have a
                 specific format and length restriction. The majority of
                 them are written in the English language. As relatively
                 few works have paid attention to sentiment analysis in
                 the Russian and Kazakh languages, this article
                 thoroughly analyzes news posts in the Kazakhstan media
                 space. The amassed datasets include texts labeled
                 according to three sentiment classes: positive,
                 negative, and neutral. The datasets are highly
                 imbalanced, with a significant predominance of the
                 positive class. Three resampling techniques
                 (undersampling, oversampling, and synthetic minority
                 oversampling (SMOTE)) are used to resample the datasets
                 to deal with this issue. Subsequently, the texts are
                 vectorized with the TF-IDF metric and classified with
                 seven machine learning (ML) algorithms: naïve Bayes,
                 support vector machine, logistic regression, k-nearest
                 neighbors, decision tree, random forest, and XGBoost.
                 Experimental results reveal that oversampling and SMOTE
                 with logistic regression, decision tree, and random
                 forest achieve the best classification scores. These
                 models are effectively employed in the developed social
                 analytics platform.",
  doi          = "10.32604/cmc.2021.017827",
}

@InProceedings{inproceedings,
  author       = "Jianhua Tao",
  year         = "2004",
  month        = "10",
  title        = "Context based emotion detection from text input",
  doi          = "10.21437/Interspeech.2004-329",
}

@Misc{mikolov2013word2vec,
  doi          = "10.48550/ARXIV.1301.3781",
  URL          = "https://arxiv.org/abs/1301.3781",
  author       = "Tomas Mikolov and Kai Chen and Greg Corrado and
                 Jeffrey Dean",
  keywords     = "Computation and Language (cs.CL), FOS: Computer and
                 information sciences, FOS: Computer and information
                 sciences",
  title        = "Efficient Estimation of Word Representations in Vector
                 Space",
  publisher    = "arXiv",
  year         = "2013",
  copyright    = "arXiv.org perpetual, non-exclusive license",
}

@Article{bert,
  author       = "Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and
                 Kristina Toutanova",
  title        = "{BERT:} Pre-training of Deep Bidirectional
                 Transformers for Language Understanding",
  journal      = "CoRR",
  volume       = "abs/1810.04805",
  year         = "2018",
  URL          = "http://arxiv.org/abs/1810.04805",
  archiveprefix = "arXiv",
  eprint       = "1810.04805",
  timestamp    = "Tue, 30 Oct 2018 20:39:56 +0100",
  biburl       = "https://dblp.org/rec/journals/corr/abs-1810-04805.bib",
  bibsource    = "dblp computer science bibliography, https://dblp.org",
}

@Article{DistilBERT,
  title        = "Distil{BERT}, a distilled version of {BERT}: smaller,
                 faster, cheaper and lighter",
  author       = "Victor Sanh and Lysandre Debut and Julien Chaumond and
                 Thomas Wolf",
  journal      = "ArXiv",
  year         = "2019",
  volume       = "abs/1910.01108",
}

@InProceedings{perezalmendros2022semeval,
  title        = "{SemEval-2022 Task 4: Patronizing and Condescending
                 Language Detection}",
  author       = "Carla P{\'e}rez-Almendros and Luis Espinosa-Anke and
                 Steven Schockaert",
  booktitle    = "Proceedings of the 16th International Workshop on
                 Semantic Evaluation (SemEval-2022)",
  year         = "2022",
  publisher    = "Association for Computational Linguistics",
}

@Article{huggingface,
  author       = "Thomas Wolf and Lysandre Debut and Victor Sanh and
                 Julien Chaumond and Clement Delangue and Anthony Moi
                 and Pierric Cistac and Tim Rault and R{\'{e}}mi Louf
                 and Morgan Funtowicz and Jamie Brew",
  title        = "HuggingFace's Transformers: State-of-the-art Natural
                 Language Processing",
  journal      = "CoRR",
  volume       = "abs/1910.03771",
  year         = "2019",
  URL          = "http://arxiv.org/abs/1910.03771",
  eprinttype   = "arXiv",
  eprint       = "1910.03771",
  timestamp    = "Tue, 02 Jun 2020 12:49:01 +0200",
  biburl       = "https://dblp.org/rec/journals/corr/abs-1910-03771.bib",
  bibsource    = "dblp computer science bibliography, https://dblp.org",
}

@Article{hichreichter,
  author       = "Sepp Hochreiter and Jürgen Schmidhuber",
  title        = "{Long Short-Term Memory}",
  journal      = "Neural Computation",
  volume       = "9",
  number       = "8",
  pages        = "1735--1780",
  year         = "1997",
  month        = "11",
  abstract     = "{Learning to store information over extended time
                 intervals by recurrent backpropagation takes a very
                 long time, mostly because of insufficient, decaying
                 error backflow. We briefly review Hochreiter's (1991)
                 analysis of this problem, then address it by
                 introducing a novel, efficient, gradient based method
                 called long short-term memory (LSTM). Truncating the
                 gradient where this does not do harm, LSTM can learn to
                 bridge minimal time lags in excess of 1000
                 discrete-time steps by enforcing constant error flow
                 through constant error carousels within special units.
                 Multiplicative gate units learn to open and close
                 access to the constant error flow. LSTM is local in
                 space and time; its computational complexity per time
                 step and weight is O. 1. Our experiments with
                 artificial data involve local, distributed,
                 real-valued, and noisy pattern representations. In
                 comparisons with real-time recurrent learning, back
                 propagation through time, recurrent cascade
                 correlation, Elman nets, and neural sequence chunking,
                 LSTM leads to many more successful runs, and learns
                 much faster. LSTM also solves complex, artificial
                 long-time-lag tasks that have never been solved by
                 previous recurrent network algorithms.}",
  ISSN         = "0899-7667",
  doi          = "10.1162/neco.1997.9.8.1735",
  URL          = "https://doi.org/10.1162/neco.1997.9.8.1735",
  eprint       = "https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf",
}
